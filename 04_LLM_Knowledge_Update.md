# ğŸ§  NLP Recent Trends â€” Knowledge Update: Temporal Misalignment, Continual Learning, RAG  

## ğŸ§­ ì „ì²´ íë¦„ ìš”ì•½

| êµ¬ë¶„ | í•µì‹¬ ê°œë… | ì£¼ìš” ë¬¸ì œ | ëŒ€í‘œ í•´ê²° ë°©ë²• |
|------|-------------|------------|----------------|
| ğŸ•°ï¸ **Temporal Misalignment** | ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ëª¨ë¸ ì§€ì‹ì´ ì˜¤ë˜ë˜ì–´ ì •í™•ë„ í•˜ë½ | ëª¨ë¸ì´ ìµœì‹  ì •ë³´ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•¨ | Continual Learning, RAG |
| ğŸ’¥ **Catastrophic Forgetting** | ìƒˆ ì •ë³´ë¥¼ í•™ìŠµí•˜ë©´ ê¸°ì¡´ ì§€ì‹ì„ ìŠìŒ | ìˆœì°¨ í•™ìŠµ ì‹œ ê¸°ì¡´ íƒœìŠ¤í¬ ì„±ëŠ¥ ì €í•˜ | Regularization / Rehearsal / Parameter Expansion |
| ğŸ” **Continual Learning** | ê³¼ê±° ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìƒˆë¡œìš´ ì§€ì‹ ìŠµë“ | íš¨ìœ¨ì  ì§€ì‹ ì—…ë°ì´íŠ¸ | EWC, LoRA, Mix-review |
| ğŸ” **Retrieval-Augmented Generation (RAG)** | ì™¸ë¶€ ë°ì´í„° ê²€ìƒ‰ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„± | ìµœì‹ ì„± í™•ë³´ ë° Hallucination ì™„í™” | Dense Retrieval, GopherCite, WebGPT ë“± |

---

## **1ï¸âƒ£ Temporal Misalignment â€” ì‹œê°„ ë¶ˆì¼ì¹˜ ë¬¸ì œ**

> LLMì€ í•œ ì‹œì ì˜ ë°ì´í„°ë§Œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì—, ì‹œê°„ì´ ì§€ë‚˜ë©´ ì„¸ìƒê³¼ì˜ â€œì§€ì‹ ê²©ì°¨â€ê°€ ìƒê¹ë‹ˆë‹¤.

### ğŸ”¹ ì •ì˜
- **Temporal Misalignment**ë€ ëª¨ë¸ì´ **ê³ ì •ëœ ì‹œì ì˜ ë°ì´í„°**ë¡œ í•™ìŠµë˜ì–´,  
  ì‹œê°„ì´ íë¥´ë©´ì„œ **ìƒˆë¡œìš´ ì‚¬ì‹¤ì´ë‚˜ ë³€í™”ëœ ì •ë³´**ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

### ğŸ”¹ ì˜ˆì‹œ
- 2017ë…„ ë°ì´í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ì´ â€œëŒ€í•œë¯¼êµ­ ëŒ€í†µë ¹ì€ ëˆ„êµ¬ì¸ê°€?â€ì— ëŒ€í•´  
  ì—¬ì „íˆ **ë¬¸ì¬ì¸**ì´ë¼ê³  ë‹µí•˜ëŠ” í˜„ìƒ.
- ì¦‰, **â€œì‹œê°„ ì¶•ì—ì„œì˜ ì¼ë°˜í™” ì‹¤íŒ¨(Temporal Generalization Gap)â€**.

### ğŸ”¹ ì‹¤í—˜ ê²°ê³¼ (Lazaridou et al., *NeurIPS 2021*)
| ëª¨ë¸ | í•™ìŠµ ê¸°ê°„ | í‰ê°€ ê¸°ê°„ | ê²°ê³¼ (Perplexity) |
|-------|------------|------------|-------------------|
| ì‹¤í—˜êµ° | ~2017 | 2018â€“2019 | ë†’ì€ Perplexity |
| ëŒ€ì¡°êµ° | ~2019 | 2018â€“2019 | ë‚®ì€ Perplexity |

> â‡’ **Temporal alignment**ê°€ ì¡´ì¬í•˜ë©°, í•™ìŠµ ì‹œì ê³¼ í‰ê°€ ì‹œì ì˜ ì°¨ì´ê°€ í´ìˆ˜ë¡ ì •í™•ë„ ì €í•˜.

---

## **2ï¸âƒ£ Catastrophic Forgetting â€” ë§ê° ë¬¸ì œ**

> ìƒˆë¡­ê²Œ Fine-tuningí•  ë•Œ ì´ì „ì— ë°°ìš´ ì§€ì‹ì„ ìŠëŠ” í˜„ìƒ.

### ğŸ”¹ ì •ì˜
- ì—¬ëŸ¬ íƒœìŠ¤í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•  ë•Œ, ì´ì „ íƒœìŠ¤í¬ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§€ëŠ” í˜„ìƒ.  
- â€œìƒˆë¡œìš´ ì§€ì‹ì„ ë°°ìš´ ëŒ€ê°€ë¡œ ê³¼ê±° ì§€ì‹ì„ ìƒì‹¤í•˜ëŠ” ë¬¸ì œ.â€

### ğŸ”¹ ì˜ˆì‹œ
- CIFAR-10 ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì„ í•™ìŠµ í›„ MNISTì— Fine-tuningí•˜ë©´,  
  CIFAR-10 ë¬¸ì œë¥¼ ë” ì´ìƒ í•´ê²°í•˜ì§€ ëª»í•¨ (Zhai et al., *CPAL 2024*).

### ğŸ”¹ ì˜í–¥
- LLMì— ìµœì‹  ì •ë³´ë¥¼ í•™ìŠµì‹œí‚¤ë”ë¼ë„,  
  â€œì—­ì‚¬ì /ë³€í•˜ì§€ ì•ŠëŠ” ì •ë³´(ì˜ˆ: 1+1=2)â€ê¹Œì§€ í•¨ê»˜ ìŠì„ ìˆ˜ ìˆìŒ.

---

## **3ï¸âƒ£ Continual Learning â€” ì§€ì†ì  í•™ìŠµ**

> Catastrophic Forgettingì„ ë°©ì§€í•˜ë©° **ìƒˆë¡œìš´ ì§€ì‹**ì„ ê¾¸ì¤€íˆ í¡ìˆ˜í•˜ëŠ” ë°©ë²•ë¡ .

---

### âš™ï¸ 3.1 Continual Learning ë°©ë²•ë¡ 

| ì ‘ê·¼ ë°©ì‹ | í•µì‹¬ ì•„ì´ë””ì–´ | ëŒ€í‘œ ë°©ë²• | íŠ¹ì§• |
|------------|----------------|-------------|-------|
| ğŸ§© **Regularization Method** | ê¸°ì¡´ íŒŒë¼ë¯¸í„° ë³€ê²½ ìµœì†Œí™” | EWC, Rec-Adam | ê¸°ì¡´ ì§€ì‹ ë³´ì¡´ ì¤‘ì‹¬ |
| ğŸ§  **Parameter Expansion Method** | ìƒˆë¡œìš´ ëª¨ë“ˆë§Œ í•™ìŠµ | LoRA, K-Adapter | ê¸°ì¡´ ì§€ì‹ ìœ ì§€ + í™•ì¥ |
| ğŸ” **Rehearsal Method** | ê³¼ê±° ë°ì´í„° ì¼ë¶€ ì¬ì‚¬ìš© | Mix-Review, DGR | ì‹¤ ë°ì´í„° ê¸°ë°˜ ë³µìŠµ |

---

### ğŸ“˜ 3.1.1 Regularization Method  
> ê¸°ì¡´ íŒŒë¼ë¯¸í„°ê°€ ì§€ë‚˜ì¹˜ê²Œ ë°”ë€Œì§€ ì•Šë„ë¡ ì œì•½ì„ ë‘ëŠ” ë°©ì‹

- **Elastic Weight Consolidation (EWC)**  
  - ê° íŒŒë¼ë¯¸í„°ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚° í›„, ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ë³€ê²½ì— íŒ¨ë„í‹° ë¶€ì—¬  
  - Kirkpatrick et al., *PNAS 2017*  
- **Rec-Adam**  
  - Adam ì˜µí‹°ë§ˆì´ì €ì— **â€œê¸°ì¡´ íŒŒë¼ë¯¸í„° ë³µì› í•­â€** ì¶”ê°€  
  - Chen et al., *EMNLP 2020*  

---

### âš™ï¸ 3.1.2 Parameter Expansion Method  
> ê¸°ì¡´ íŒŒë¼ë¯¸í„°ë¥¼ ê·¸ëŒ€ë¡œ ë‘ê³ , ìƒˆë¡œìš´ ì •ë³´ìš© ëª¨ë“ˆë§Œ ì¶”ê°€ í•™ìŠµ

- **LoRA (Low-Rank Adaptation)**  
  - ê°€ì¤‘ì¹˜ì˜ ì¼ë¶€ë¥¼ ì €ì°¨ì› í–‰ë ¬ë¡œ ë¶„í•´í•´ ì¶”ê°€ í•™ìŠµ  
  - Hu et al., *ICLR 2022*  
- **K-Adapter**  
  - ì™¸ë¶€ ì§€ì‹ì„ ë…ë¦½ì ì¸ ì–´ëŒ‘í„°ë¡œ ì£¼ì…í•˜ì—¬ ì§€ì‹ ì „ì´ ê°•í™”  
  - Wang et al., *ACL 2021*

---

### ğŸ” 3.1.3 Rehearsal Method  
> Fine-tuning ì‹œ ì´ì „ í•™ìŠµ ë°ì´í„°ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ â€œê¸°ì–µ ìœ ì§€â€

- **Mix-Review (He et al., *EACL 2021*)**  
  - Fine-tuningí•  ë•Œ ì¼ë¶€ Pretraining ë°ì´í„°ë¥¼ ì„ì–´ í•™ìŠµ  
  - Weight Decay ê¸°ë°˜ë³´ë‹¤ NLL(ì†ì‹¤) ìœ ì§€ê°€ ë›°ì–´ë‚¨  
- **Deep Generative Replay (DGR)**  
  - ì´ì „ íƒœìŠ¤í¬ì˜ ë°ì´í„°ë¥¼ ìƒì„± ëª¨ë¸ë¡œ ë³µì›í•˜ì—¬ ë³µìŠµì— í™œìš©  

---

### ğŸ” 3.2 Continual Learningì˜ í•œê³„

| í•œê³„ì  | ì„¤ëª… |
|---------|-------|
| ğŸ’¸ ìì› ë¹„ìš© | ë§¤ì¼ ìƒˆë¡œ í•™ìŠµí•˜ë ¤ë©´ ë§‰ëŒ€í•œ GPU/ì‹œê°„ í•„ìš” |
| ğŸ§­ Alignment ë¶ˆì•ˆì • | RLHFë‚˜ DPO ë“±ìœ¼ë¡œ ì¡°ì •ëœ ëª¨ë¸ì— ì¬í•™ìŠµ ì‹œ ì•ˆì •ì„± ì €í•˜ |
| â³ ì‹œì˜ì„± í•œê³„ | ìì£¼ í•™ìŠµí•´ë„ â€œë°©ê¸ˆ ì¼ì–´ë‚œ ì‚¬ê±´â€ì—” ì—¬ì „íˆ ë°˜ì‘ ë¶ˆê°€ |

> â— ì¦‰, **â€œëª¨ë¸ì„ ì¬í•™ìŠµí•˜ì§€ ì•Šê³  ìµœì‹  ì§€ì‹ì„ ë°˜ì˜í•  ë°©ë²•â€**ì´ í•„ìš” â†’ **RAG**

---

## **4ï¸âƒ£ Retrieval-Augmented Generation (RAG)**

> LLMì„ ë§¤ë²ˆ ë‹¤ì‹œ í•™ìŠµì‹œí‚¤ëŠ” ëŒ€ì‹ , ì™¸ë¶€ ì§€ì‹ ì†ŒìŠ¤ë¡œë¶€í„°  
> **í•„ìš”í•œ ì •ë³´ë¥¼ ê²€ìƒ‰(Retrieval)**í•˜ì—¬ **Promptì— í†µí•©(In-context)**í•˜ëŠ” ë°©ë²•.

---

### ğŸ”¹ 4.1 ì™¸ë¶€ ì •ë³´ í™œìš©í•˜ê¸°

- ìµœì‹  ë‰´ìŠ¤, ë‚´ë¶€ ë¬¸ì„œ, ê³¼ê±° ëŒ€í™” ë“±ì—ì„œ **ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰ í›„ Promptì— ì‚½ì…**  
- íš¨ê³¼:
  - Temporal Misalignment ì™„í™” (ìµœì‹ ì„± ë³´ê°•)
  - Hallucination ê°ì†Œ (ê·¼ê±° ê¸°ë°˜ ì‘ë‹µ)
- í™œìš© ì˜ˆ:
  - ê²€ìƒ‰ ì—”ì§„
  - ì‚¬ë‚´ DB (ë©”ì¼, ë¬¸ì„œ, ë³´ê³ ì„œ)
  - ì‚¬ìš©ì ì„¸ì…˜ íˆìŠ¤í† ë¦¬

---

### ğŸ” Retrieval ê¸°ë²•

| ê¸°ë²• | ì„¤ëª… | ëŒ€í‘œ ì—°êµ¬ |
|------|------|------------|
| **TF-IDF / BM25** | ì „í†µì  ë¬¸ì„œ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ | â€” |
| **Sentence-BERT** | ë¬¸ì¥ ì„ë² ë”© ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ | Reimers & Gurevych, *EMNLP 2019* |
| **Dense Passage Retrieval (DPR)** | Query & Passageë¥¼ ë…ë¦½ ì¸ì½”ë”ë¡œ ì„ë² ë”© í›„ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ | Karpukhin et al., *EMNLP 2020* |

> **Faiss / Pinecone** ë“±ì„ í™œìš©í•œ ANN(Approximate Nearest Neighbor) ê²€ìƒ‰ìœ¼ë¡œ íš¨ìœ¨ í–¥ìƒ  
> https://github.com/facebookresearch/faiss  
> https://www.pinecone.io/

---

### ğŸ§© 4.2 NaÃ¯ve RAGì˜ í•œê³„

| ë¬¸ì œ | ì„¤ëª… | ì—°êµ¬ |
|-------|------|------|
| âŒ Retrieval ì˜¤ë¥˜ | ì˜ëª»ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê±°ë‚˜ Noise í¬í•¨ | Mallen et al., *ACL 2023* |
| âš–ï¸ Prior vs Context | ëª¨ë¸ì˜ ê¸°ì¡´ ì§€ì‹ì´ Retrieval ë¬¸ì„œì™€ ì¶©ëŒ ì‹œ Prior ìš°ì„¸ | Longpre et al., *EMNLP 2021* |
| ğŸ“Š Popularity Bias | ìœ ëª…í•œ ì‚¬ì‹¤ì€ LLMì´ ë” ì˜ ì•Œê³ , RAGëŠ” ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ | Mallen et al., *ACL 2023* |

> ì¦‰, RAGëŠ” **ëœ ì•Œë ¤ì§„(Long-tail)** ì •ë³´ì—ì„œë§Œ ìœ íš¨ì„±ì´ ë†’ìŒ.

---

### ğŸ§  4.3 ì‹¬í™” RAG ë°©ë²•ë¡ 

#### ğŸ“˜ RAG (Lewis et al., *NeurIPS 2020*)
- ìµœì´ˆë¡œ â€œRetrieval-Augmented Generationâ€ ê°œë… ì œì•ˆ  
- Retriever + Generatorë¥¼ **End-to-End í•™ìŠµ**  
- ê° ë¬¸ì„œ ê¸°ë°˜ í† í° í™•ë¥ ì„ **Marginalize**í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±.

#### ğŸ” GopherCite (Menick et al., *arXiv 2022*)
- Googleì˜ 280B ëª¨ë¸ **Gopher** ê¸°ë°˜  
- ê²€ìƒ‰ì—”ì§„ì„ í†µí•´ **ì›¹í˜ì´ì§€ ì¸ìš© ê·¼ê±°**ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€ ìƒì„±  
- RLHFë¡œ â€œë‹µë³€ê³¼ ì¸ìš© ê°„ ì •í•©ì„±â€ì„ ê°•í™”  
- ê´€ë ¨ ì—°êµ¬: **WebGPT (OpenAI)**, **BlenderBot3 (Meta)**

---

## **5ï¸âƒ£ í•µì‹¬ ì •ë¦¬**

| êµ¬ë¶„ | ë¬¸ì œ | ì£¼ìš” ì ‘ê·¼ë²• | ëŒ€í‘œ ì—°êµ¬ |
|------|------|--------------|-------------|
| ğŸ•°ï¸ Temporal Misalignment | ì‹œê°„ ê²½ê³¼ë¡œ ì¸í•œ ì§€ì‹ ë…¸í›„í™” | Continual Learning, RAG | Lazaridou et al., 2021 |
| ğŸ’¥ Catastrophic Forgetting | ê³¼ê±° ì§€ì‹ ìƒì‹¤ | Regularization, Rehearsal | Kirkpatrick et al., 2017 |
| ğŸ” Continual Learning | ì§€ì†ì  ì§€ì‹ ì—…ë°ì´íŠ¸ | LoRA, Mix-Review, EWC | Jin et al., 2022 |
| ğŸ” RAG | ì™¸ë¶€ì •ë³´ ê¸°ë°˜ ì‹¤ì‹œê°„ ë³´ì™„ | Dense Retrieval, GopherCite | Lewis et al., 2020 |

---

## ğŸ§© ê²°ë¡  ë° ì „ë§

- **LLMì˜ ì§€ì‹ ì—…ë°ì´íŠ¸ ë¬¸ì œëŠ” ë¶ˆê°€í”¼** â€” ì„¸ìƒì€ ëŠì„ì—†ì´ ë³€í•˜ê¸° ë•Œë¬¸.  
- **Continual Learning**ì€ ëª¨ë¸ ë‚´ë¶€ì˜ ì¥ê¸°ì  ì§€ì‹ ë³´ì¡´ì„,  
  **RAG**ëŠ” ì‹¤ì‹œê°„ ì™¸ë¶€ ì§€ì‹ í™œìš©ì„ ë‹´ë‹¹í•˜ëŠ” **ë³´ì™„ì  ê´€ê³„**ì— ìˆìŒ.  
- ì•ìœ¼ë¡œì˜ ë°©í–¥ì€:
  - **Hybrid Knowledge Update (CL + RAG)**  
  - **Efficient Unlearning & Selective Updating**  
  - **Dynamic Retrieval with Real-time Context Adaptation**

---

ğŸ“š **ì°¸ê³  ë…¼ë¬¸**
- Lazaridou et al., *Mind the Gap: Temporal Generalization in LMs*, NeurIPS 2021  
- Jin et al., *Lifelong Pretraining*, ACL 2022  
- Hu et al., *LoRA*, ICLR 2022  
- Lewis et al., *Retrieval-Augmented Generation*, NeurIPS 2020  
- Menick et al., *GopherCite*, arXiv 2022  
- Mallen et al., *When Not to Trust LMs*, ACL 2023  

---

ğŸ§¾ **ì¶œì²˜:**  
[NLP Recent Trends] Knowledge Update (NAVER Connect Foundation, 2024)  
ë¹„ì˜ë¦¬ì  êµìœ¡ ëª©ì ì— í•œí•´ ìš”ì•½ ë° ì¬êµ¬ì„±ë¨.
