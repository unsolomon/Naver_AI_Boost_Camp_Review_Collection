# ğŸ§© ì „ì²´ íë¦„ ìš”ì•½

| êµ¬ë¶„ | ì£¼ìš” ë°©ë²• | í•µì‹¬ ì•„ì´ë””ì–´ | ì¥ì  |
|------|-------------|----------------|------|
| ğŸ§  Chain-of-Thought | ë‹¨ê³„ë³„ Reasoning | ëª…ì‹œì  ì‚¬ê³ ê³¼ì • ìœ ë„ | ë³µì¡í•œ ë¬¸ì œ í•´ê²° |
| âš™ï¸ Zero-Shot CoT | ì˜ˆì‹œ ì—†ëŠ” ì¶”ë¡  | â€œLetâ€™s think step by stepâ€ | ë‹¨ì¼ Promptë¡œ Reasoning |
| ğŸ” Self-Consistency | ë‹¤ì¤‘ Reasoning ë³‘í•© | Sampling ê¸°ë°˜ Ensemble | ì •í™•ë„ í–¥ìƒ |
| ğŸ§© Least-to-Most | ì‘ì€ ë‹¨ê³„ë¶€í„° í•´ê²° | ë‹¨ê³„ì  ë¬¸ì œ ë¶„í•´ | ê³„ì‚°/ì¶”ë¡  ë¬¸ì œì— ê°•í•¨ |
| ğŸ§± Decomposed Prompting | Sub-task ëª¨ë“ˆí™” | êµ¬ì¡°ì  ë¶„í•´ | ì¼ë°˜í™” ì„±ëŠ¥ ìš°ìˆ˜ |
| ğŸŒ ReAct | Reasoning + Acting ê²°í•© | ì™¸ë¶€ ì§€ì‹ í™œìš© | ë„êµ¬ ê¸°ë°˜ ì¶”ë¡  ê°€ëŠ¥ |

---


## 1ï¸âƒ£ LLMì€ ìƒê°ì„ í• ê¹Œ?

### ğŸ”¸ LLMì˜ í•œê³„
- LLMì€ **Auto-Regressive ëª¨ë¸** â†’ â€œë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡â€í•˜ëŠ” êµ¬ì¡°.
- ì¦‰, **â€˜ìƒê°â€™**ì´ë¼ê¸°ë³´ë‹¤ëŠ” **íŒ¨í„´ ì¸ì‹ ê¸°ë°˜ í™•ë¥ ì  ì˜ˆì¸¡**.
- ì˜ˆ: `232 - 197 + 37 = ?`  
  â†’ LLMì´ ì‹¤ì œë¡œ ê³„ì‚°í•œ ê²ƒì´ ì•„ë‹ˆë¼,  
  â€œ72â€ë¼ëŠ” ë‹¨ì–´ê°€ ë‹¤ìŒì— ë‚˜ì˜¬ í™•ë¥ ì´ ë†’ì•˜ê¸° ë•Œë¬¸ì— ì¶œë ¥.

### ğŸ”¸ í•µì‹¬ í¬ì¸íŠ¸
- LLM ìì²´ëŠ” **ì‚¬ê³ (Thinking)** ëŠ¥ë ¥ì„ ê°€ì§€ì§€ ì•ŠìŒ.
- í•˜ì§€ë§Œ â€œìƒê°í•˜ëŠ” ê³¼ì •ì„ í‰ë‚´ë‚´ë„ë¡â€ í•™ìŠµ/ìœ ë„í•  ìˆ˜ ìˆìŒ â†’ **Reasoning & Planning**ì˜ í•„ìš”ì„±.

---

## 2ï¸âƒ£ ìƒê°í•´ë³´ê¸°

### ğŸ§© 2.1 Chain-of-Thought (CoT)

> Wei et al., *Chain-of-Thought Prompting Elicits Reasoning in LLMs*, NeurIPS 2022

- ë¬¸ì œ í•´ê²° ê³¼ì •ì„ **ìì—°ì–´ë¡œ ì„œìˆ **í•˜ê²Œ í•˜ì—¬, ë‹¨ê³„ì  Reasoningì„ ìœ ë„.
- **ë³µì¡í•œ ë¬¸ì œ(ì‚°ìˆ˜, ìƒì‹, ê¸°í˜¸ ì¶”ë¡ )**ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„.
- íŠ¹íˆ **ëŒ€ê·œëª¨ ëª¨ë¸(>100B)**ì—ì„œ **Emergent Ability**ë¡œ ë‚˜íƒ€ë‚¨.

#### âœ… íŠ¹ì§•
- ìì—°ì–´ í˜•íƒœì˜ ì„¤ëª…ì´ í¬í•¨ë˜ì–´ì•¼ íš¨ê³¼ì .
- ë‹¨ìˆœ ìˆ˜ì‹/ê²°ê³¼ë§Œ ì ëŠ” ê²ƒì€ íš¨ê³¼ ì—†ìŒ.
- â€œë‹µ â†’ ì´ìœ â€ êµ¬ì¡°ë³´ë‹¤ â€œì´ìœ  â†’ ë‹µâ€ êµ¬ì¡°ê°€ íš¨ê³¼ì .
- ì½”ë“œ ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì¼ìˆ˜ë¡ Reasoning ì„±ëŠ¥ ìš°ìˆ˜.

---

### ğŸ§© 2.2 Zero-Shot Chain-of-Thought

> Kojima et al., *Large Language Models are Zero-Shot Reasoners*, NeurIPS 2022

- **ì˜ˆì‹œ ì—†ì´(CoT ì˜ˆì‹œ ì œê³µ ì—†ì´)**ë„ ìŠ¤ìŠ¤ë¡œ Reasoning ê°€ëŠ¥.
- `"Let's think step by step"` ê³¼ ê°™ì€ ê°„ë‹¨í•œ íŠ¸ë¦¬ê±° ë¬¸êµ¬ë¡œë„ ì‘ë™.

#### âœ… íŠ¹ì§•
- NaÃ¯ve Zero-Shotë³´ë‹¤ ì •í™•ë„ê°€ ë†’ìŒ.
- ë³µì¡í•œ ë¬¸ì œ(GSM8K ë“±)ì—ì„œ íŠ¹íˆ ì„±ëŠ¥ í–¥ìƒ.
- **Zero-Plus-Few-Shot CoT**:  
  Zero-shotìœ¼ë¡œ ìƒì„±ëœ ì˜ˆì‹œë¥¼ ë‹¤ì‹œ Few-shot CoTë¡œ í™œìš© ì‹œ ì •í™•ë„ ìƒìŠ¹.
- **Emergent Ability**: í° ëª¨ë¸ì—ì„œë§Œ ìœ ì˜ë¯¸í•œ íš¨ê³¼ ë°œìƒ.

---

### ğŸ§© 2.3 Self-Consistency

> Wang et al., *Self-Consistency Improves CoT Reasoning in LLMs*, ICLR 2023

- CoTì˜ **ë‹¨ì¼ Reasoning ê²½ë¡œ ì˜¤ë¥˜** ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê¸°ë²•.
- ì—¬ëŸ¬ Reasoning ê²½ë¡œë¥¼ ìƒ˜í”Œë§í•˜ì—¬ **ë‹¤ìˆ˜ê²°ë¡œ ìµœì¢… ë‹µ ê²°ì •**.
- ì¼ì¢…ì˜ **Ensemble ë°©ì‹**.

#### âœ… íŠ¹ì§•
- ë‹¤ì–‘í•œ ëª¨ë¸(GPT-3, UL2, LaMDA, PaLM ë“±)ì—ì„œ íš¨ê³¼ ê²€ì¦.
- **Beam searchë³´ë‹¤ Sampling ê¸°ë°˜ ì¶”ë¡ ì´ ì •í™•ë„ ë†’ìŒ.**
- Temperatureë¥¼ ë†’ì—¬ ë‹¤ì–‘ì„±ì„ í™•ë³´í• ìˆ˜ë¡ ì„±ëŠ¥ í–¥ìƒ.
- Sample ìˆ˜ ì¦ê°€ â†’ ì •í™•ë„ ìƒìŠ¹ í›„ ìˆ˜ë ´.

---

## 3ï¸âƒ£ ì–´ë ¤ìš´ ë¬¸ì œë¥¼ ì‘ì€ ë¬¸ì œë¡œ ë‚˜ëˆ„ê¸°

### ğŸ§© 3.1 Least-to-Most Prompting

> Zhou et al., *Least-to-Most Prompting Enables Complex Reasoning in LLMs*, ICLR 2023

- ë³µì¡í•œ ë¬¸ì œë¥¼ **ë‹¨ê³„ì ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ í•´ê²°**í•˜ëŠ” ì ‘ê·¼.
- â€œì‰¬ìš´ ë¬¸ì œ â†’ ì ì°¨ ì–´ë ¤ìš´ ë¬¸ì œâ€ ìˆœìœ¼ë¡œ ì¶”ë¡ .

#### âœ… íŠ¹ì§•
- Chain-of-Thoughtë³´ë‹¤ ë³µì¡í•œ ê³„ì‚°/ê¸°í˜¸ ì¶”ë¡  ê³¼ì œì—ì„œ ìš°ìˆ˜.
- ë‹¨ê³„ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ CoTë³´ë‹¤ ë” ë†’ì€ ì •í™•ë„.
- Few-shot ì˜ˆì‹œì˜ í˜•íƒœê°€ ë‹¬ë¼ì ¸ë„ ì¼ë°˜í™” ì„±ëŠ¥ ìœ ì§€.
- **ì½”ë“œ ë°ì´í„° í•™ìŠµ ëª¨ë¸**ì—ì„œ ì„±ëŠ¥ì´ ë” ì¢‹ìŒ.

---

### ğŸ§© 3.2 Decomposed Prompting

> Khot et al., *Decomposed Prompting: A Modular Approach for Solving Complex Tasks*, ICLR 2023

- ë¬¸ì œë¥¼ **Sub-task ë‹¨ìœ„ë¡œ ë¶„í•´**í•˜ê³ ,  
  **Decomposer â†” Sub-task handler** ê°„ ìƒí˜¸ì‘ìš©ì„ í†µí•´ í•´ê²°.
- ë³µì¡í•œ ì‘ì—…ì„ **ëª¨ë“ˆì‹**ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” êµ¬ì¡°.

#### âœ… íŠ¹ì§•
- Least-to-Mostë³´ë‹¤ ë” ë†’ì€ **ì •í™•ë„ì™€ ì¼ë°˜í™” ì„±ëŠ¥**.
- **ê³„ì¸µì  / ì¬ê·€ì  ë¶„í•´** ë°©ì‹ìœ¼ë¡œ ê¸´ ì‹œí€€ìŠ¤ì—ë„ ê°•í•¨.
- ì™¸ë¶€ ë„êµ¬ ë˜ëŠ” í•¨ìˆ˜ í˜¸ì¶œê³¼ ì—°ê³„ ê°€ëŠ¥.

---

## 4ï¸âƒ£ ì™¸ë¶€ ê¸°ëŠ¥ì„ í™œìš©í•œ Reasoning

### ğŸ§© 4.1 ReAct (Reason + Act)

> Yao et al., *ReAct: Synergizing Reasoning and Acting in LLMs*, ICLR 2023

- LLMì´ **Reasoning(ì¶”ë¡ )**ê³¼ **Acting(í–‰ë™, ì¦‰ ì™¸ë¶€ë„êµ¬ ì‚¬ìš©)**ì„ ë™ì‹œì— ìˆ˜í–‰í•˜ë„ë¡ ì„¤ê³„.

#### ğŸ§  ì£¼ìš” ê¸°ëŠ¥
| í•¨ìˆ˜ | ì„¤ëª… |
|------|------|
| `Search[entity]` | entityì˜ ìœ„í‚¤í˜ì´ì§€ ì²« 5ë¬¸ì¥ ë°˜í™˜ (ì—†ì„ ê²½ìš° ìœ ì‚¬ entity ë°˜í™˜) |
| `Lookup[string]` | ë¬¸ìì—´ í¬í•¨ ë¬¸ì¥ ê²€ìƒ‰ (Ctrl+F ìœ ì‚¬) |
| `Finish[answer]` | ì •ë‹µ ë°˜í™˜ í›„ ì¢…ë£Œ |

#### âœ… íŠ¹ì§•
- ë‹¨ë… ReActëŠ” ì •í™•ë„ê°€ ë‚®ì•„ **Self-Consistency**ì™€ í•¨ê»˜ ì‚¬ìš©.
- ë°˜ë³µ ê²€ìƒ‰/ì˜¤ë¥˜ ê²€ìƒ‰ ì‹œ ì˜¤ë‹µ ìœ ë°œ ê°€ëŠ¥.
- **CoT-SC â†” ReAct** ìƒí˜¸ë³´ì™„ì  ì‚¬ìš©:
  - CoT-SC ì‹¤íŒ¨ ì‹œ â†’ ReAct ì‚¬ìš©  
  - ReAct ì‹¤íŒ¨ ì‹œ â†’ CoT-SC ëŒ€ì²´
- **Fine-tuning ë°©ì‹**ì´ In-contextë³´ë‹¤ ë†’ì€ ì •í™•ë„.
- ì•½ 3,000ê°œ Sampleë¡œ í•™ìŠµ.

---


## ğŸ§­ í•µì‹¬ Takeaway

- LLMì€ â€œìƒê°í•˜ëŠ”â€ ëª¨ë¸ì´ ì•„ë‹ˆë¼ â€œíŒ¨í„´ ì˜ˆì¸¡â€ ëª¨ë¸ì´ì§€ë§Œ,  
  **Prompt ì„¤ê³„ì— ë”°ë¼ Reasoning ëŠ¥ë ¥ì„ ìœ ë„í•  ìˆ˜ ìˆìŒ.**
- **CoT â†’ Self-Consistency â†’ Decomposition â†’ ReAct**ë¡œ ì´ì–´ì§€ëŠ”  
  íë¦„ì€ **â€œë‹¨ìˆœ íŒ¨í„´ ì¸ì‹ â†’ ë³µí•©ì  ì¶”ë¡  + í–‰ë™â€**ìœ¼ë¡œì˜ ë°œì „ì„ ì˜ë¯¸.
- í–¥í›„ LLMì˜ ë°œì „ ë°©í–¥ì€ **Reasoning, Planning, Acting**ì˜ ìœµí•©.

---

ğŸ“š **ì°¸ê³  ë…¼ë¬¸**
- Wei et al., *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models*, NeurIPS 2022  
- Kojima et al., *Large Language Models are Zero-Shot Reasoners*, NeurIPS 2022  
- Wang et al., *Self-Consistency Improves CoT Reasoning in LLMs*, ICLR 2023  
- Zhou et al., *Least-to-Most Prompting Enables Complex Reasoning in LLMs*, ICLR 2023  
- Khot et al., *Decomposed Prompting: A Modular Approach for Solving Complex Tasks*, ICLR 2023  
- Yao et al., *ReAct: Synergizing Reasoning and Acting in LLMs*, ICLR 2023

---

ğŸ§¾ **ì¶œì²˜:**  
[NLP Recent Trends] Reasoning & Planning (NAVER Connect Foundation, 2024)  
ë¹„ì˜ë¦¬ì  êµìœ¡ ëª©ì ì— í•œí•´ ìš”ì•½ ë° ì¬êµ¬ì„±ë¨.
