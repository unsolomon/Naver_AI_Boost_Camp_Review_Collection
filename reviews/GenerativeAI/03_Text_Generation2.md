# ğŸ§  3ê°•: Text Generation 2 â€” Parameter Efficient Tuning (PEFT)
> ê°•í•„ì„± êµìˆ˜ (ì„œìš¸ëŒ€í•™êµ ì‚°ì—…ê³µí•™ê³¼)  
> ì¶œì²˜: NAVER Connect Foundation ê°•ì˜ êµì¬  
> ì°¸ê³  ë…¼ë¬¸: Lialin et al. (2023), Hu et al. (2021), Houlsby et al. (2019)

---

## **1. LLM ë°œì „ê³¼ PEFT ë°©ë²•ë¡ ì˜ ì¤‘ìš”ì„±**

### ğŸ”¹ 1.1 LLMì˜ ë°œì „ ê³¼ì •

- **Transformer ê¸°ë°˜ ì–¸ì–´ëª¨ë¸**ì€ 2017ë…„ ì´í›„ NLP ì „ë°˜ì—ì„œ íšê¸°ì  ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜´  
- **2020ë…„ ì´í›„:** â€œGeneral-purpose LLMâ€ì˜ ë“±ì¥  
  - ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ë‹¨ì¼ ëª¨ë¸ë¡œ ìˆ˜í–‰ ê°€ëŠ¥  
  - í•™ìŠµ ë°ì´í„° ë° ëª¨ë¸ íŒŒë¼ë¯¸í„° í¬ê¸°ê°€ ì¦ê°€í• ìˆ˜ë¡ ì„±ëŠ¥ í–¥ìƒ

ğŸ“ˆ *ì°¸ê³  ë…¼ë¬¸:*  
- *A Survey of Large Language Models* (Zhao et al., 2023)  
- *Predictability and Surprise in Large Generative Models* (Ganguli et al., 2022)

---

### âš™ ì–¸ì–´ëª¨ë¸ í™œìš©ì˜ ì „í†µì  ì ˆì°¨

1. **Pre-training**: ëŒ€ê·œëª¨ ë²”ìš© ì›¹ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì „í•™ìŠµ  
2. **Fine-tuning**: Downstream task(ìš”ì•½, ë¶„ë¥˜, QA ë“±)ì— ë§ê²Œ íŒŒì¸íŠœë‹  

â¡ï¸ íŠ¹ì • íƒœìŠ¤í¬ ì„±ëŠ¥ì€ ë†’ì§€ë§Œ, ìì› íš¨ìœ¨ì„±ê³¼ ì§€ì† í•™ìŠµì—ëŠ” í•œê³„ ì¡´ì¬

---

## **2. ê¸°ì¡´ í•™ìŠµ ë°©ë²•ë¡  (Conventional Approaches)**

### ğŸ”¸ Fine-Tuning (ì „í†µì  ë°©ë²•)
| ì ‘ê·¼ ë°©ì‹ | ì„¤ëª… | ì„±ëŠ¥ | íš¨ìœ¨ì„± |
|------------|------|------|--------|
| **Full fine-tuning** | ëª¨ë“  íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ | ìµœê³  | ë‚®ìŒ |
| **Partial fine-tuning** | ì¶œë ¥ ë ˆì´ì–´ë§Œ ì—…ë°ì´íŠ¸ | ì¤‘ê°„ | ì¤‘ê°„ |
| **Feature extraction** | ì„ë² ë”©ë§Œ ì¶”ì¶œ í›„ ë¶„ë¥˜ê¸° í•™ìŠµ | ë‚®ìŒ | ë†’ìŒ |

> ì „ì²´ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì‹œ ì„±ëŠ¥ì€ ë†’ì§€ë§Œ, í•™ìŠµÂ·ì €ì¥Â·ë°°í¬ ë¹„ìš©ì´ ë§¤ìš° í¼:contentReference[oaicite:0]{index=0}.

---

### ğŸ”¸ In-Context Learning (ICL)
- GPT-3 ì´í›„ ì œì•ˆëœ ì ‘ê·¼ë²•  
- ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ì • ì—†ì´ **few-shot prompting**ìœ¼ë¡œ íƒœìŠ¤í¬ ìˆ˜í–‰  
- Prompt ë‚´ì— *Demonstration + Test Input*ì„ í•¨ê»˜ ì…ë ¥

ğŸ“˜ *ì°¸ê³ :* *Language Models are Few-Shot Learners* (Brown et al., 2020)

---

## **3. ê¸°ì¡´ ë°©ë²•ë¡ ì˜ í•œê³„**

### âš  ì„±ëŠ¥ì  í•œê³„
- **Catastrophic Forgetting:**  
  ìƒˆë¡œìš´ íƒœìŠ¤í¬ í•™ìŠµ ì‹œ ê¸°ì¡´ ì§€ì‹ì„ ìƒëŠ” í˜„ìƒ  
  (*Chen et al., 2020 â€“ Recall and Learn*)  
- ëª¨ë¸ì„ ì§€ì†ì ìœ¼ë¡œ fine-tuningí•˜ë©´ ì´ì „ ë°ì´í„° ê¸°ì–µë ¥ ì €í•˜

### âš  ìì›ì  í•œê³„
- ëª¨ë¸ í¬ê¸°ê°€ ìˆ˜ì‹­ì–µ íŒŒë¼ë¯¸í„°ë¡œ ì»¤ì§€ë©´ì„œ ì „ì²´ í•™ìŠµì´ ë¶ˆê°€ëŠ¥  
- Downstream taskë³„ ë…ë¦½ ëª¨ë¸ ì €ì¥Â·ë°°í¬ì— ë§‰ëŒ€í•œ ìì› ì†Œëª¨  
  (*Lifearchitect.ai í†µê³„ ì°¸ê³ *)

### âš  ì‹ ë¢°ì„± í•œê³„
- ICLì€ prompt êµ¬ì„±ì— ë”°ë¼ ì„±ëŠ¥ì´ ë“¤ì‘¥ë‚ ì‘¥  
- **ëœë¤ ë ˆì´ë¸”**ì„ ë„£ì–´ë„ ì¼ì • ì„±ëŠ¥ì„ ë³´ì´ëŠ” í˜„ìƒ ì¡´ì¬  
  (*Min et al., 2022 â€“ Rethinking the Role of Demonstrations*)

---

## **4. Parameter-Efficient Fine-Tuning (PEFT) ê°œìš”**

### ğŸ§© ì •ì˜
> ëª¨ë¸ì˜ **ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ**í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ë¡ 

- ê¸°ì¡´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ëŠ” **ë™ê²°(freeze)**  
- ì ì€ ìˆ˜ì˜ **ì¶”ê°€ í•™ìŠµ íŒŒë¼ë¯¸í„°**ë§Œ ì—…ë°ì´íŠ¸  
- Fine-tuning ì‹œ ë©”ëª¨ë¦¬Â·ì‹œê°„ ì ˆê°

ğŸ“— *ì°¸ê³ :* *Scaling Down to Scale Up: A Guide to PEFT* (Lialin et al., 2023)

---

### ğŸ”¸ PEFT ì£¼ìš” ëª©í‘œ

| ëª©í‘œ | ì„¤ëª… |
|------|------|
| **Parameter Efficiency** | ì ì€ í•™ìŠµ íŒŒë¼ë¯¸í„°ë¡œ ë™ì¼ ì„±ëŠ¥ ë‹¬ì„± |
| **Computation Efficiency** | ë©”ëª¨ë¦¬ ë° GPU ìì› ì ˆê° |
| **Task Adaptability** | ë‹¤ì–‘í•œ downstream taskì— ì†ì‰½ê²Œ ì ìš© |
| **Model Integrity** | ì› ëª¨ë¸ êµ¬ì¡° ìœ ì§€, ì•ˆì •ì„± ë³´ì¥ |

---

## **5. PEFT ì£¼ìš” ë°©ë²•ë¡ **

### âš™ï¸ 1. Adapter (Houlsby et al., 2019)

- **êµ¬ì¡°:**  
  ê° Transformer ë ˆì´ì–´ì— **Feed-Forward Network (FFN)**ì„ ì¶”ê°€  
  â†’ ì…ë ¥ì„ ë‚®ì€ ì°¨ì›ìœ¼ë¡œ ì••ì¶• â†’ ë¹„ì„ í˜• ë³€í™˜ â†’ ì›ë˜ ì°¨ì› ë³µì›  
- **í•µì‹¬:** ë³‘ëª©(bottleneck) êµ¬ì¡°  
- **Fine-tuning:** Adapter ëª¨ë“ˆë§Œ í•™ìŠµ, ë‚˜ë¨¸ì§€ëŠ” ê³ ì •  

ğŸ“Š *ì„±ê³¼:*  
GLUE benchmark 9ê°œ taskì—ì„œ **Full Fine-tuningì— ê·¼ì ‘í•œ ì„±ëŠ¥**,  
í•˜ì§€ë§Œ í•™ìŠµ íŒŒë¼ë¯¸í„°ëŠ” ìˆ˜ì‹­ ë°° ê°ì†Œ.

---

### âš™ï¸ 2. Prefix Tuning (Li et al., 2021)

- Transformer ê° ë ˆì´ì–´ì— **trainable prefix vector** ì¶”ê°€  
- PrefixëŠ” â€œê°€ìƒì˜ ì„ë² ë”©â€ìœ¼ë¡œ ê°„ì£¼ë˜ì–´ ê¸°ì¡´ ëª¨ë¸ì— ë³‘í•© ê°€ëŠ¥  
- ê° íƒœìŠ¤í¬ë³„ ìµœì  prefixë¥¼ í•™ìŠµí•˜ì—¬ ë¹ ë¥¸ ì ì‘ ê°€ëŠ¥  

> ğŸ”¹ ëª¨ë¸ì˜ attention layer ì•ë‹¨ì— prefix vectorë¥¼ ì£¼ì…í•˜ëŠ” ë°©ì‹

---

### âš™ï¸ 3. Prompt Tuning (Lester et al., 2021)

- ì…ë ¥ ë ˆì´ì–´ì— **trainable prompt embedding** ì¶”ê°€  
- â€œìì—°ì–´ promptâ€ê°€ ì•„ë‹Œ **ì„ë² ë”© ê³µê°„ìƒì˜ ê°€ìƒ ë²¡í„°**  
- ì„ë² ë”© ë ˆì´ì–´ ì¼ë¶€ë§Œ í•™ìŠµ  
- ì†Œí˜• ëª¨ë¸ì— íŠ¹íˆ íš¨ê³¼ì 

ğŸ“˜ *ì°¨ì´ì :*  
Prompt Tuningì€ ì…ë ¥ì¸µë§Œ ìˆ˜ì •,  
Prefix Tuningì€ ëª¨ë“  Transformer ë ˆì´ì–´ì— prefixë¥¼ ì¶”ê°€í•¨.

---

### âš™ï¸ 4. Low-Rank Adaptation (LoRA) â€” Hu et al., 2021

- ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” **ê³ ì •(freeze)**  
- ê° weight matrix \( W \)ì— ëŒ€í•´,  
  **ì €ë­í¬ í–‰ë ¬ ë¶„í•´ (Low-Rank Decomposition)**ì„ ì¶”ê°€:
  \[
  W' = W + BA
  \]
  (A: down-projection, B: up-projection, rank â‰ª dimension)

- **í•µì‹¬ ì•„ì´ë””ì–´:**  
  íŒŒë¼ë¯¸í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì¡°ì •í•˜ë©´ì„œ ê¸°ì¡´ ëª¨ë¸ ì„±ëŠ¥ ìœ ì§€  
- **íŠ¹ì§•:**  
  - Inference latency ì¦ê°€ ì—†ìŒ  
  - ëª¨ë¸ êµ¬ì¡° ë³€ê²½ ë¶ˆí•„ìš”  
  - Encoder/Decoder ê¸°ë°˜ ëª¨ë¸ ëª¨ë‘ ì ìš© ê°€ëŠ¥ (RoBERTa, GPT ë“±)  
  - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ì ˆê°  

ğŸ“Š *ì„±ëŠ¥:*  
- GPT-3 (175B)ì—ì„œë„ íš¨ê³¼ ì…ì¦  
- ê¸°ì¡´ Fine-tuning ëŒ€ë¹„ **95% ì´ìƒì˜ íŒŒë¼ë¯¸í„° ì ˆê°**  
- **PEFT ì¤‘ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë¨**

---

## ğŸ§  6. PEFT ì‹¤ì œ ì ìš©

- **Hugging Face `peft` ë¼ì´ë¸ŒëŸ¬ë¦¬**ë¥¼ í†µí•´ ê°„í¸í•˜ê²Œ êµ¬í˜„ ê°€ëŠ¥  
  ```python
  from peft import LoraConfig, get_peft_model
  model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")
  peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05)
  model = get_peft_model(model, peft_config)


## âœ… ê²°ë¡ 

  | ìš”ì•½ í•­ëª©      | ë‚´ìš©                                          |
| ---------- | ------------------------------------------- |
| **ê¸°ì¡´ ë¬¸ì œì ** | ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµì˜ ë¹„íš¨ìœ¨ì„±, ìì› ë‚­ë¹„, ì‹ ë¢°ì„± ë¬¸ì œ             |
| **í•´ê²° ë°©í–¥**  | ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµí•˜ëŠ” PEFT ë°©ì‹ ë„ì…                    |
| **í•µì‹¬ ë°©ë²•ë¡ ** | Adapter, Prefix Tuning, Prompt Tuning, LoRA |
| **ëŒ€í‘œ ê¸°ë²•**  | LoRA â€” íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•ì                          |
| **íš¨ê³¼**     | ë™ì¼ ì„±ëŠ¥ì„ í›¨ì”¬ ì ì€ ìì›ìœ¼ë¡œ êµ¬í˜„, LLM í™œìš©ë„ ê·¹ëŒ€í™”           |


ğŸ“š ì°¸ê³  ë¬¸í—Œ

Zhao et al., A Survey of Large Language Models, 2023
Ganguli et al., Predictability and Surprise in Large Generative Models, 2022
Chen et al., Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting, 2020
Min et al., Rethinking the Role of Demonstrations in In-Context Learning, 2022
Houlsby et al., Parameter-Efficient Transfer Learning for NLP, 2019
Li et al., Prefix-Tuning: Optimizing Continuous Prompts for Generation, 2021
Lester et al., The Power of Scale for Parameter-Efficient Prompt Tuning, 2021
Hu et al., LoRA: Low-Rank Adaptation of Large Language Models, 2021
Lialin et al., Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning, 2023



