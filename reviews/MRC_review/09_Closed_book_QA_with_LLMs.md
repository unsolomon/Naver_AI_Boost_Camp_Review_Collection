## ğŸ“˜ 09_Closed_book_QA_with_LLMs.md

## ğŸ§­ ì „ì²´ ìš”ì•½

| êµ¬ë¶„ | í•µì‹¬ ë‚´ìš© | ì‹¤ë¬´/í”„ë¡œì íŠ¸ ì ìš© í¬ì¸íŠ¸ |
|------|------------|-----------------------------|
| Closed-book QA | ì™¸ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ì—†ì´ ì–¸ì–´ëª¨ë¸ ìì²´ì˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ | ì‚¬ì „í•™ìŠµ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ êµ¬í˜„ ì‹œ ì ìš© |
| LLM & Zero-shot | í•™ìŠµ ë°ì´í„° ì—†ì´ë„ ì¼ë°˜í™”ëœ ë¬¸ì œ í•´ê²° ê°€ëŠ¥ | ì œë¡œìƒ· QA, Few-shot Prompt ì„¤ê³„ ì°¸ê³  |
| LoRA | íš¨ìœ¨ì  Fine-tuning ê¸°ë²• | GPU ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½ì—ì„œ LLM íŠœë‹ |
| ì‹¤í—˜ê²°ê³¼ | T5, LLaMA ê¸°ë°˜ CBQAì˜ ê°•ì  ë° í•œê³„ | Text-to-Text QA íŒŒì´í”„ë¼ì¸ ì„¤ê³„ ì‹œ ì°¸ê³  |

---

## 1ï¸âƒ£ Closed-book Question Answering (CBQA)

### ğŸ”¹ ê°œë…
- **Open-book QA**:  
  ì™¸ë¶€ ì§€ì‹(ì˜ˆ: Wikipedia)ì„ ë¬¸ì„œ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  ê²€ìƒ‰ í›„ ë‹µë³€ ìƒì„±.  
  â†’ ì¥ì : ì‹¤ì‹œê°„ ì •ë³´ ë°˜ì˜ ê°€ëŠ¥  
  â†’ ë‹¨ì : ê²€ìƒ‰ ë° ìŠ¤í† ë¦¬ì§€ ë¹„ìš©ì´ í¼  
- **Closed-book QA**:  
  ëª¨ë¸ì´ **ì‚¬ì „ í•™ìŠµ ê³¼ì •ì—ì„œ ì´ë¯¸ ì§€ì‹ì„ ë‚´ì¬**í•˜ê³  ìˆë‹¤ê³  ê°€ì •.  
  ì™¸ë¶€ ê²€ìƒ‰ ì—†ì´ ëª¨ë¸ ë‚´ë¶€ì˜ íŒŒë¼ë¯¸í„°ë§Œìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±.

```text
Input:  "Who discovered penicillin?"
Output: "Alexander Fleming"
```

> âœ… í•µì‹¬:  
> Closed-book QA = â€œRetrieval-free knowledge utilizationâ€

### ğŸ“š ì‹¤í—˜ì  ê·¼ê±°
- ì–¸ì–´ëª¨ë¸ì´ ìì²´ì ìœ¼ë¡œ **ì‚¬ì‹¤ ê¸°ë°˜ ì§€ì‹(factual knowledge)** ì„ ë³´ìœ í•œë‹¤ëŠ” ì—°êµ¬ ê²°ê³¼ ì¡´ì¬ (ì˜ˆ: Petroni et al., 2019)
- GPT-2, GPT-3 ë“±ì€ **zero-shot QA**ì—ì„œë„ ì¼ì • ìˆ˜ì¤€ì˜ ì •í™•í•œ ë‹µë³€ì„ ìƒì„± ê°€ëŠ¥

---

## 2ï¸âƒ£ Large Language Models & Zero-shot Performance

### âš™ï¸ Task-specific Fine-tuningì˜ í•œê³„
1. **ë¼ë²¨ë§ ë¹„ìš©**ì´ ë†’ìŒ  
2. **Pre-trained knowledge**ê°€ ì†ìƒë  ìœ„í—˜ ì¡´ì¬  
3. **Scaling Law**ì— ë”°ë¼ ëª¨ë¸ í¬ê¸° í™•ì¥ì´ ì–´ë ¤ì›€

â†’ í•´ê²°ì±…:  
**â€œZero-shot Learning + In-context Learningâ€**

### ğŸ’¡ Text-to-Text Framework
ëª¨ë“  NLP taskë¥¼ ì…ë ¥â†’ì¶œë ¥ì˜ **í…ìŠ¤íŠ¸ ë³€í™˜ ë¬¸ì œ**ë¡œ í†µí•©  
ì˜ˆ:  
- `translate English to Korean: hello` â†’ â€œì•ˆë…•â€
- `question: What is AI?` â†’ â€œArtificial Intelligenceâ€

> ëŒ€í‘œ ëª¨ë¸: **T5 (Text-to-Text Transfer Transformer)**  
> - T5-base: 220M  
> - T5-XXL: 11B  
> - ë‹¤ì–‘í•œ NLP ë¬¸ì œë¥¼ ë‹¨ì¼ í”„ë ˆì„ì›Œí¬ë¡œ ì²˜ë¦¬ ê°€ëŠ¥

### ğŸ§  LLMì˜ ì§€ì‹ í•™ìŠµ ë°©ì‹
- **SkipGram** / **BERT Masked LM** / **GPT Next-token prediction** / **T5 Span corruption**
- ê²°ê³¼ì ìœ¼ë¡œ ëª¨ë¸ì´ íŒŒë¼ë¯¸í„° ë‚´ë¶€ì— â€œì„¸ê³„ ì§€ì‹â€ì„ ì €ì¥

### ğŸ’¬ In-context Learning
- ì—­ì „íŒŒ ì—†ì´ **ì…ë ¥ ë‚´ ì˜ˆì‹œ(few-shot prompt)** ë§Œìœ¼ë¡œ ìƒˆë¡œìš´ task ìˆ˜í–‰  
- GPT-3, Llama, ChatGPT ë“±ì€ ì´ ëŠ¥ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì œë¡œìƒ· QA ìˆ˜í–‰ ê°€ëŠ¥

> â€œëª¨ë¸ì´ ì´ë¯¸ ì„¸ìƒì„ ë°°ì› ê¸° ë•Œë¬¸ì—, ìƒˆë¡œìš´ ì˜ˆì‹œë§Œ ì£¼ë©´ ë°”ë¡œ ì´í•´í•œë‹¤.â€

---

## 3ï¸âƒ£ LoRA (Low-Rank Adaptation)

### ğŸ§© ë“±ì¥ ë°°ê²½
- LLMì˜ Fine-tuning ë¹„ìš©ì´ ë§¤ìš° í¬ê¸° ë•Œë¬¸ì—, ì „ì²´ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ì§€ ì•Šê³  **ì¼ë¶€ë§Œ ì¡°ì •**í•˜ëŠ” íš¨ìœ¨ì  ê¸°ë²• í•„ìš”.

### ğŸ”‘ í•µì‹¬ ê°œë…
| êµ¬ë¶„ | ê¸°ì¡´ Fine-tuning | LoRA |
|------|------------------|------|
| í•™ìŠµ ë²”ìœ„ | ì „ì²´ íŒŒë¼ë¯¸í„° | ì¼ë¶€ ì €ì°¨ì› ë¶€ë¶„ë§Œ |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ë†’ìŒ | ë§¤ìš° ë‚®ìŒ |
| ì†ë„ | ëŠë¦¼ | ë¹ ë¦„ |
| ì„±ëŠ¥ | ìœ ì§€ or ì•½ê°„ í–¥ìƒ | ìœ ì§€ or ê°œì„  |

- LoRAëŠ” `W â‰ˆ Wâ‚€ + AÂ·B` í˜•íƒœë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë¶„í•´í•˜ì—¬, **A, Bë§Œ í•™ìŠµ**
- Hugging Faceì˜ `peft` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ì‰½ê²Œ ì ìš© ê°€ëŠ¥

> âœ… LoRA = â€œëŒ€í˜• ì–¸ì–´ëª¨ë¸ì„ ê°€ë³ê²Œ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê¸°ìˆ â€

---

## 4ï¸âƒ£ Experiment Result & Analysis

### âš™ï¸ Experimental Setting
- **ë°ì´í„°ì…‹**: Open-domain QA / MRC ë°ì´í„°ì—ì„œ Context ì œê±°  
- **Salient Span Masking (SSM)**: ê³ ìœ ëª…ì‚¬Â·ë‚ ì§œ ë“± ì˜ë¯¸ ë‹¨ìœ„ë¥¼ ë§ˆìŠ¤í‚¹ í›„ í•™ìŠµ  
- **Fine-tuning**: Pre-trained T5 ëª¨ë¸ë¡œ ì¶”ê°€ í•™ìŠµ  
  - Input: `trivia question: <ì§ˆë¬¸>`  
  - Output: `answer: <ë‹µë³€1> answer: <ë‹µë³€2>`

### ğŸ“Š ê²°ê³¼ ìš”ì•½
| ëª¨ë¸ | íŠ¹ì§• | ì„±ëŠ¥ |
|------|------|------|
| **T5 (XXL)** | Context ì—†ëŠ” Closed-book QA | ë†’ì€ ì •í™•ë„ |
| **T5 + SSM** | ì¤‘ìš” í† í° ë§ˆìŠ¤í‚¹ ì¶”ê°€ | ì„±ëŠ¥ í¬ê²Œ í–¥ìƒ |
| **LLaMA (7B~75B)** | ì†Œí˜• ëª¨ë¸ë„ ëŒ€í˜• LLM ì„±ëŠ¥ ê·¼ì ‘ | íš¨ìœ¨ì  êµ¬ì¡° |

> Salient Span Masking ê¸°ë²•ì€ **ì§€ì‹ ì¼ë°˜í™”(knowledge generalization)** ëŠ¥ë ¥ì„ ê°•í™”í•¨.

### ğŸš¨ False Negative ì‚¬ë¡€
1. **Phrasing mismatch** â€“ í‘œí˜„ë§Œ ë‹¤ë¦„  
2. **Incomplete annotation** â€“ ì¼ë¶€ ì •ë‹µ ëˆ„ë½  
3. **Unanswerable** â€“ ì‹œì /ë§¥ë½ ì˜ì¡´ì  ì§ˆë¬¸

### âš ï¸ í•œê³„ì 
- ëŒ€í˜• ëª¨ë¸ì¼ìˆ˜ë¡ ì†ë„Â·ë©”ëª¨ë¦¬ ë¬¸ì œ  
- ëª¨ë¸ ë‚´ë¶€ ì§€ì‹ì˜ **ì¶œì²˜ ë¶ˆëª…** (ì„¤ëª… ê°€ëŠ¥ì„± ë‚®ìŒ)  
- ìƒˆë¡œìš´ ì§€ì‹ ì—…ë°ì´íŠ¸ ì–´ë ¤ì›€

---

### ğŸ”— Reference
- Raffel et al. (2020) *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer*  
- Roberts et al. (2020) *How Much Knowledge Can You Pack Into the Parameters of a Language Model?*  
- Petroni et al. (2019) *Language Models as Knowledge Bases?*  
- Radford et al. (2019) *Language Models are Unsupervised Multitask Learners*

---
