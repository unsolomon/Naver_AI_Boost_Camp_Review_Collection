## âœ… Generation-based MRC í•µì‹¬ ìš”ì•½

| ë‹¨ê³„           | ë‚´ìš©                            | ë¹„ê³                         |
| ------------ | ----------------------------- | ------------------------- |
| **1. ì •ì˜**    | ë‹µë³€ì„ ì§ì ‘ ìƒì„±í•˜ëŠ” MRC               | Seq2Seq êµ¬ì¡°                |
| **2. ëª¨ë¸**    | BART / T5 / mT5               | Encoderâ€“Decoder           |
| **3. ì „ì²˜ë¦¬**   | Tokenization + Special Tokens | `[SOS]`, `[EOS]`, `[PAD]` |
| **4. í•™ìŠµ ëª©í‘œ** | í…ìŠ¤íŠ¸ ì¬êµ¬ì„± / ë‹µë³€ ìƒì„±               | Cross-Entropy Loss        |
| **5. í›„ì²˜ë¦¬**   | Beam Searchë¡œ ìµœì  ì‹œí€€ìŠ¤ ì„ íƒ        | Auto-regressive           |
| **6. í‰ê°€**    | EM / F1 / BLEU / ROUGE        | í‘œí˜„ ë‹¤ì–‘ì„± ê³ ë ¤                 |

---

## 1ï¸âƒ£ Generation-based MRC ê°œìš”

### ğŸ§  ë¬¸ì œ ì •ì˜

MRC(Machine Reading Comprehension)ëŠ” ì£¼ì–´ì§„ **ì§€ë¬¸(Context)** ê³¼ **ì§ˆë¬¸(Question)** ì„ ê¸°ë°˜ìœ¼ë¡œ **ë‹µë³€(Answer)** ì„ ë„ì¶œí•˜ëŠ” ê³¼ì œë‹¤.

ì´ë¥¼ í‘¸ëŠ” ì ‘ê·¼ì€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤.

| ë°©ì‹                       | ì„¤ëª…                             | ìœ í˜•                 | ëŒ€í‘œ ëª¨ë¸                 |
| ------------------------ | ------------------------------ | ------------------ | --------------------- |
| **Extraction-based MRC** | ì •ë‹µì´ ì§€ë¬¸ ë‚´ **ì¼ì • êµ¬ê°„(span)** ìœ¼ë¡œ ì¡´ì¬ | ë¶„ë¥˜(Classification) | BERT, ALBERT, ELECTRA |
| **Generation-based MRC** | ì •ë‹µì„ **ì§€ë¬¸ì„ ë³´ê³  ìƒˆë¡­ê²Œ ìƒì„±**          | ìƒì„±(Generation)     | BART, T5, mT5         |

ì¦‰,

* **Extraction-based** â†’ "ì§€ë¬¸ ì† ì •ë‹µ ìœ„ì¹˜ë¥¼ ì°¾ì•„ë¼"
* **Generation-based** â†’ "ì§€ë¬¸ê³¼ ì§ˆë¬¸ì„ ë³´ê³  ì •ë‹µì„ ì§ì ‘ ë§í•´ë¼"

> âœ… *í•µì‹¬:*
> Generation-based MRCëŠ” â€œí…ìŠ¤íŠ¸ ìƒì„±â€ ë¬¸ì œë¡œ ì ‘ê·¼í•˜ë©°,
> ëª¨ë¸ì´ ì •ë‹µì„ ì§ì ‘ ë§Œë“¤ì–´ë‚´ê¸° ë•Œë¬¸ì— **ììœ ë„ê°€ ë†’ì§€ë§Œ ì •ë‹µ í‰ê°€ê°€ ì–´ë µë‹¤.**

---

## 2ï¸âƒ£ Generation-based MRC í‰ê°€ ë°©ë²•

Extraction ê¸°ë°˜ê³¼ ë™ì¼í•˜ê²Œ **Exact Match (EM)** ì™€ **F1 Score** ë¥¼ ì‚¬ìš©í•œë‹¤.

* **EM (Exact Match)** : ì˜ˆì¸¡ ë‹µì´ ì •ë‹µê³¼ ì™„ì „íˆ ë™ì¼í•œ ê²½ìš°ë§Œ 1ì 
* **F1 Score** : ì˜ˆì¸¡ ë‹µê³¼ ì •ë‹µ ê°„ **token overlap ë¹„ìœ¨**ë¡œ ê³„ì‚°

> ì„œìˆ í˜• ë‹µë³€ì˜ ë‹¤ì–‘ì„±ì„ ê³ ë ¤í•  ë• **BLEU / ROUGE-L** ë“±ì˜ ìƒì„± í’ˆì§ˆ ì§€í‘œë„ í™œìš© ê°€ëŠ¥.

---

## 3ï¸âƒ£ Generation-based MRC Overview

1. **Step 1 â€” Tokenization**

   * ì§€ë¬¸ê³¼ ì§ˆë¬¸ì„ í† í° ë‹¨ìœ„ë¡œ ë¶„í•´
2. **Step 2 â€” Model Input**

   * í† í°í™”ëœ ì…ë ¥ì„ Generation ê¸°ë°˜ MRC ëª¨ë¸(BART, T5 ë“±)ì— ì „ë‹¬
3. **Step 3 â€” Answer Generation**

   * ëª¨ë¸ì´ ë‹µë³€ ë¬¸ì¥ì„ ìƒì„± (ì˜ˆ: â€œë¯¸êµ­ ìœ¡êµ°ë¶€ì°¸ëª¨ì´ì¥â€)

---

## 4ï¸âƒ£ Extraction vs Generation ë¹„êµ ìš”ì•½

| êµ¬ë¶„         | Extraction-based       | Generation-based       |
| ---------- | ---------------------- | ---------------------- |
| **ë¬¸ì œ í˜•íƒœ**  | ë¶„ë¥˜(Classification)     | ìƒì„±(Generation)         |
| **ì…ì¶œë ¥ êµ¬ì¡°** | PLM + Classifier       | Seq2Seq PLM            |
| **ì •ë‹µ í˜•íƒœ**  | ì§€ë¬¸ ë‚´ span              | ììœ  í…ìŠ¤íŠ¸(Free-form text) |
| **ì¶œë ¥ ë‹¨ìœ„**  | start/end ìœ„ì¹˜           | ë¬¸ì¥ ì „ì²´ (í† í° ì‹œí€€ìŠ¤)         |
| **ëŒ€í‘œ ëª¨ë¸**  | BERT, RoBERTa, ELECTRA | BART, T5, mT5          |
| **í‰ê°€ ì§€í‘œ**  | EM / F1                | EM / F1 / ROUGE / BLEU |
| **ì¥ì **     | ëª…í™•í•œ í‰ê°€, ë¹ ë¦„             | ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ìƒì„±, ë²”ìš©ì„±       |
| **ë‹¨ì **     | ì •ë‹µì´ ì§€ë¬¸ ë‚´ ìˆì–´ì•¼ í•¨         | ìƒì„± ë‹¤ì–‘ì„±ìœ¼ë¡œ í‰ê°€ ì–´ë ¤ì›€        |

> ğŸ’¡ *ì‹¤ì „ Tip:*
>
> * **Extraction**ì€ â€œì‚¬ì‹¤ ê¸°ë°˜ QAâ€, â€œKorQuADâ€ ë“±ì— ì í•©
> * **Generation**ì€ â€œì„¤ëª…í˜• QAâ€, â€œì¶”ë¡ í˜• ì§ˆì˜ì‘ë‹µâ€, â€œëŒ€í™”í˜• MRCâ€ì— ì í•©

---

## 5ï¸âƒ£ Pre-processing (ì…ë ¥ ì „ì²˜ë¦¬)

### ğŸ§¾ ë°ì´í„° ì˜ˆì‹œ

```json
{
 "question": "ë¯¸êµ­ êµ°ëŒ€ ë‚´ ë‘ ë²ˆì§¸ë¡œ ë†’ì€ ì§ìœ„ëŠ” ë¬´ì—‡ì¸ê°€?",
 "answers": [{"answer_start": 204, "text": "ë¯¸êµ­ ìœ¡êµ°ë¶€ì°¸ëª¨ì´ì¥"}],
 "id": "6521755-0-0"
}
```

* **Extraction-based MRC**: `answer_start` ìœ„ì¹˜ë¥¼ ì‚¬ìš©í•´ spanì„ ì˜ˆì¸¡
* **Generation-based MRC**: ì •ë‹µ **textë§Œ ìˆìœ¼ë©´ ì¶©ë¶„**

---

### ğŸ”  Tokenization

> í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ê³  ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •

* **WordPiece Tokenizer** ì‚¬ìš©
* ì‚¬ì „ì— ë“±ë¡ëœ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
* â€œ##â€ì€ **ì• ë‹¨ì–´ì— ì´ì–´ì§€ëŠ” ë¶€ë¶„(subword)** ì„ ì˜ë¯¸

ì˜ˆì‹œ:

```
ë¬¸ì¥: "ë¯¸êµ­êµ°ëŒ€ë‚´ë‘ë²ˆì§¸ë¡œë†’ì€ì§ìœ„ëŠ”ë¬´ì—‡ì¸ê°€?"
í† í°í™”: [â€˜ë¯¸êµ­â€™, â€˜êµ°ëŒ€â€™, â€˜ë‚´â€™, â€˜ë‘ë²ˆì§¸â€™, â€˜##ë¡œâ€™, â€˜ë†’ì€â€™, â€˜ì§â€™, â€˜##ìœ„ëŠ”â€™, â€˜ë¬´ì—‡ì¸ê°€â€™, â€˜?â€™]
ì¸ë±ìŠ¤ ë³€í™˜: [101, 23545, 8910, 2456, 5678, ...]
```

---

### ğŸ§© Special Tokens

> ëª¨ë¸ì´ ì…ë ¥ êµ¬ì¡°ë¥¼ ì´í•´í•˜ë„ë¡ ë„ì™€ì£¼ëŠ” íŠ¹ìˆ˜ í† í°

| Token             | ì˜ë¯¸         | Generation-based MRC ì‚¬ìš© |
| ----------------- | ---------- | ----------------------- |
| **[CLS]**         | ë¬¸ì¥ ì‹œì‘      | ì„ íƒì                      |
| **[SEP]**         | ë¬¸ì¥ êµ¬ë¶„      | ì„ íƒì                      |
| **[PAD]**         | ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶¤  | âœ… í•„ìˆ˜                    |
| **[SOS] / [EOS]** | ë¬¸ì¥ ì‹œì‘/ë í‘œì‹œ | âœ… ì‚¬ìš© (ë””ì½”ë” ì…ë ¥ìš©)          |

> ğŸ§  Extractionì€ `[CLS]`, `[SEP]`, `[PAD]` ì‚¬ìš©
> ğŸ§  Generationì€ `[SOS]`, `[EOS]` ì¤‘ì‹¬ì˜ ìì—°ì–´ í¬ë§·ìœ¼ë¡œ ì…ë ¥ êµ¬ì„±

---

### ğŸ§® ì¶”ê°€ ì…ë ¥ ì •ë³´ (Additional Info)

| í•­ëª©                 | ì—­í•            | Generation ê¸°ë°˜ íŠ¹ì§•  |
| ------------------ | ------------ | ----------------- |
| **Attention Mask** | ì—°ì‚°í•  í† í° ì—¬ë¶€ ì§€ì • | Extractionê³¼ ë™ì¼    |
| **Token Type IDs** | ì…ë ¥ êµ¬ë¶„ìš© ID    | âŒ BART ë“±ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ |
| **Padding**        | ì‹œí€€ìŠ¤ ê¸¸ì´ ë³´ì •    | âœ… í•„ìˆ˜              |

> âš ï¸ BARTëŠ” BERTì²˜ëŸ¼ Segment êµ¬ë¶„ì´ ì—†ì–´ `token_type_ids`ê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ

---

### ğŸ“ ì¶œë ¥ í‘œí˜„

* **Extraction**: ë‹µë³€ì˜ ì‹œì‘/ë ìœ„ì¹˜(span index)
* **Generation**: **ë‹µë³€ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ìƒì„±**
  â†’ ê° stepì—ì„œ ëª¨ë¸ì´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” **autoregressive decoding** êµ¬ì¡°

---

## 6ï¸âƒ£ Model (Generation-based PLMs)

### ğŸ§± BART (Bidirectional and Auto-Regressive Transformers)

> **BERT + GPT í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°**

| êµ¬ì„±                      | íŠ¹ì§•                  |
| ----------------------- | ------------------- |
| **Encoder (BERT-like)** | ë¬¸ë§¥ì„ ì–‘ë°©í–¥ìœ¼ë¡œ ì¸ì½”ë”©       |
| **Decoder (GPT-like)**  | í•œ ë°©í–¥ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„± |

**Pre-training Objective**

* ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì†ìƒì‹œí‚¤ê³ (ë§ˆìŠ¤í‚¹, ë‹¨ì–´ ì‚­ì œ, ìˆœì„œ ë’¤ì„ê¸° ë“±)
* ì›ë˜ ë¬¸ì¥ì„ ë³µêµ¬í•˜ë„ë¡ í•™ìŠµ (denoising autoencoding)

**ì¥ì **

* MRC, ìš”ì•½, ë²ˆì—­ ë“± ë‹¤ì–‘í•œ **Seq2Seq task**ì— ì‚¬ìš© ê°€ëŠ¥
* ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ë™ì‹œì— í™œìš©í•˜ì—¬ ë¬¸ë§¥ ì´í•´ + ìƒì„± ëª¨ë‘ ê°€ëŠ¥

---

### âš™ï¸ BART í•™ìŠµ ê°œë… ìš”ì•½

| ë‹¨ê³„               | ì„¤ëª…                      |
| ---------------- | ----------------------- |
| **1. Encoding**  | ì†ìƒëœ ë¬¸ì¥ì„ ì¸ì½”ë”ì— ì…ë ¥         |
| **2. Decoding**  | ì›ë˜ ë¬¸ì¥ì„ ë³µì›í•˜ë„ë¡ ìƒì„±         |
| **3. Objective** | Reconstruction Loss ìµœì†Œí™” |
| **4. ê²°ê³¼**        | ë¬¸ë§¥ ì´í•´ + ìƒì„± ëŠ¥ë ¥ ë™ì‹œ í™•ë³´     |

> ì˜ˆ: â€œê·¸ëŠ” í•™êµì— ê°”ë‹¤.â€ â†’ â€œê·¸ëŠ” [MASK]ì— ê°”ë‹¤.â€
> ëª¨ë¸ì´ â€˜í•™êµâ€™ë¥¼ ì˜¬ë°”ë¥´ê²Œ ë³µì›í•˜ë„ë¡ í•™ìŠµ

---

### ğŸ”¥ ë‹¤ë¥¸ Generation ê¸°ë°˜ ëª¨ë¸ë“¤

| ëª¨ë¸                                         | êµ¬ì¡°                    | íŠ¹ì§•                           |
| ------------------------------------------ | --------------------- | ---------------------------- |
| **T5 (Text-to-Text Transfer Transformer)** | Encoderâ€“Decoder       | ëª¨ë“  NLP taskë¥¼ â€œí…ìŠ¤íŠ¸ â†’ í…ìŠ¤íŠ¸â€ë¡œ í†µì¼ |
| **mT5**                                    | ë‹¤êµ­ì–´ ë²„ì „                | 100+ê°œ ì–¸ì–´ ì§€ì›                  |
| **Flan-T5**                                | Instruction tuning ë°˜ì˜ | ëª…ë ¹ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µì— ê°•í•¨               |

> ğŸ’¡ **T5 vs BART**
>
> * T5ëŠ” task prefix(â€œquestion: â€¦ context: â€¦â€)ë¥¼ ë¶™ì—¬ í•™ìŠµ
> * BARTëŠ” ë” ììœ ë¡œìš´ í¬ë§· ì§€ì›

---

## 7ï¸âƒ£ Post-processing (í›„ì²˜ë¦¬)

### ğŸ§­ Decoding

> ëª¨ë¸ì´ ë‹¨ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ë©´ì„œ ë¬¸ì¥ì„ ì™„ì„±í•˜ëŠ” ê³¼ì •

**Auto-regressive êµ¬ì¡°:**
ì´ì „ stepì—ì„œ ìƒì„±ëœ í† í°ì´ ë‹¤ìŒ stepì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë¨.

* ì²« ì…ë ¥: `[SOS]` ë˜ëŠ” `<s>` (ë¬¸ì¥ ì‹œì‘ í† í°)
* ì¢…ë£Œ ì¡°ê±´: `[EOS]` ìƒì„± ì‹œ ë©ˆì¶¤

---

### ğŸ” Decoding ì „ëµ (Searching Methods)

| ë°©ë²•                    | ì„¤ëª…                     | íŠ¹ì§•           |
| --------------------- | ---------------------- | ------------ |
| **Greedy Search**     | ê° ë‹¨ê³„ì—ì„œ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ë‹¨ì–´ ì„ íƒ | ë¹ ë¥´ì§€ë§Œ ìµœì  ë³´ì¥ X |
| **Exhaustive Search** | ê°€ëŠ¥í•œ ëª¨ë“  ì¡°í•© íƒìƒ‰           | ì •í™•í•˜ì§€ë§Œ ê³„ì‚°ëŸ‰ ë§ìŒ |
| **Beam Search**       | ìƒìœ„ kê°œ í›„ë³´ë¥¼ ë³‘ë ¬ íƒìƒ‰        | íš¨ìœ¨ì„±ê³¼ í’ˆì§ˆ ê· í˜•   |

> ğŸ’¡ ì‹¤ì „ì—ì„œëŠ” ëŒ€ë¶€ë¶„ **Beam Search** ì‚¬ìš©
> beam widthëŠ” 3~5ê°€ ì¼ë°˜ì 

---



## ğŸš€ ì‹¤ë¬´ ë° í”„ë¡œì íŠ¸ í™œìš© í¬ì¸íŠ¸

1. **KorQuAD** ë“±ì—ì„œ *extractive â†’ generative ë³€í™˜ fine-tuning* ì‹¤í—˜ ê°€ëŠ¥
2. **BART-base ë˜ëŠ” T5-small**ë¶€í„° ì‹œë„ (í•™ìŠµ íš¨ìœ¨â†‘)
3. **prompt í¬ë§· ì„¤ê³„**ê°€ ì„±ëŠ¥ì— ì˜í–¥ (â€œì§ˆë¬¸: â€¦ ì§€ë¬¸: â€¦ ë‹µë³€: â€¦â€)
4. **Decoding íŒŒë¼ë¯¸í„° (beam, temperature)** ì¡°ì •ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µ ìƒì„±
5. í‰ê°€ ì‹œ **F1 + ROUGE-L** ë³‘í–‰ ì¶”ì²œ

---

ğŸ“š **ì°¸ê³ **

* Lewis et al., *BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension* (ACL 2020)
* Raffel et al., *T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer* (JMLR 2020)
* Hugging Face Transformers ë¬¸ì„œ: [https://huggingface.co/docs](https://huggingface.co/docs)
