# 성능 향상 기술

------------------------------------------------------------------------
## 📌 요약

-   **모델 선별**
    -   직접 탐색, 간접 탐색
-   **하이퍼파라미터 최적화**
    -   모델 학습 과정에 영향을 주는 변수 조정
    -   WandB 등 다양한 도구 사용
-   **앙상블**
    -   다양한 모델을 결합해 최적 출력 도출
-   **PEFT**
    -   대규모 모델을 작은 디바이스에서도 활용 가능하게 만드는 기술

------------------------------------------------------------------------

## 1. 기반 모델 선택

자연어처리 문제마다 적합한 모델이 서로 다르다.

-   **직접 조사**: 보유한 데이터에 대해 직접 실험 후 최적의 모델을 선정
    (비교적 작고 가벼운 모델에 대해서 진행)
-   **간접 조사**: 유사한 데이터에 대한 외부 실험 검토, 최적의 모델 선정
    (사용하고자 하는 도메인과 유사한 벤치마크 참고)

**간접 조사 고려사항** - 언어 처리 능력 - 도메인 유사성 - 출력 형태

------------------------------------------------------------------------

## 2. 하이퍼파라미터 최적화

하이퍼파라미터: 기계 학습 모델 훈련을 관리하는 외부 구성 변수.
훈련 시 모델 가중치 업데이트에 영향을 줌.

### 주요 도구

* **Scikit-Learn**
  * GridSearchCV (모든 조합 탐색)
  * RandomizedSearchCV (랜덤 조합 탐색)
* **Hyperopt**
  * Bayesian Optimization 기반 탐색
* **Optuna**
  * Sequential Model-Based Optimization (SMBO)
* **WandB Sweep**
  * 하이퍼파라미터 탐색과 시각화 지원(설치: `!pip install wandb`)


------------------------------------------------------------------------

## 3. 앙상블 기법

- **Voting**  
  여러 모델의 예측을 투표 방식으로 결합하여 최종 예측을 결정

- **Bagging**  
  데이터를 여러 번 샘플링하여 여러 모델을 학습 후, 평균 또는 다수결로 최종 예측 결정  
  (예: Random Forest)

- **Boosting**  
  약한 학습기를 순차적으로 학습시켜 강한 학습기를 만드는 방식  
  (예: AdaBoost, Gradient Boosting)
  
---
### 응용 분야
- **N21, N2N**  
  - Voting, Bagging, Boosting 기법 적용 가능
- **N21, N2N, N2M**  
  - Average Log-probability, Router, Mixture of Experts (MoE) 기법 적용 가능
---
### 세부 기법
- **Average Log-probability**  
  여러 모델의 로그 확률을 평균내어 최종 예측 결정
- **Mixture of Experts (MoE)**  
  입력 데이터의 특정 부분을 서로 다른 전문가 모델들이 분담하여 처리
- **Router**  
  MoE 내부에서, 입력에 따라 적절한 전문가 모델을 선택하는 분류기


------------------------------------------------------------------------

## 4. Parameter Efficient Fine Tuning (PEFT)

-   **Prefix Tuning**
    -   일반 Fine tuning 대신 Prefix를 적용
    -   하나의 Task에 대해 일부 레이어만 튜닝
-   **LoRA (Low-Rank Adaptation)**
    -   Pretrained 모델 파라미터는 고정, 외부 파라미터만 학습
    -   전체 모델 재학습 없이 특정 작업에 맞게 빠르고 효과적으로 튜닝
-   **ReFT (Representation Fine-tuning)**
    -   Parameter를 직접 건드리지 않고 Hidden states 보정
    -   Hidden states에 행렬을 적용하여 추가 성능 향상 가능
    -   LoRA와 병행 가능

------------------------------------------------------------------------

