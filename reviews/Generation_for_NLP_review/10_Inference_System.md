# âš™ï¸ Lecture 10: Inference System

## 1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„± (Basic Text Generation)

### ğŸ§  LLM í…ìŠ¤íŠ¸ ìƒì„± (LLM Text Generation)
- LLMì€ **ë‹¤ìŒ ë‹¨ì–´(í† í°)** ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµëœ ëª¨ë¸ì…ë‹ˆë‹¤.
- **ë””ì½”ë”©(Decoding)** ì€ ì´ëŸ¬í•œ í™•ë¥  ë¶„í¬ì—ì„œ í† í°ì„ ì„ íƒí•´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
- ë””ì½”ë”© ë°©ì‹ì— ë”°ë¼ ê²°ê³¼ì˜ ë‹¤ì–‘ì„±ê³¼ í’ˆì§ˆì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
  - **Greedy Decoding**: í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í† í°ì„ ìˆœì°¨ì ìœ¼ë¡œ ì„ íƒ (ê²°ì •ì ì´ì§€ë§Œ ë‹¤ì–‘ì„± ë¶€ì¡±)
  - **Top-k Sampling**: í™•ë¥ ì´ ë†’ì€ ìƒìœ„ kê°œ í›„ë³´ ì¤‘ í•˜ë‚˜ë¥¼ ë¬´ì‘ìœ„ ì„ íƒ (ì°½ì˜ì  í…ìŠ¤íŠ¸ ìƒì„± ê°€ëŠ¥)
  - **Top-p (Nucleus) Sampling**: ëˆ„ì  í™•ë¥  p ì´í•˜ì˜ í›„ë³´ ì¤‘ì—ì„œ ìƒ˜í”Œë§ (ìì—°ìŠ¤ëŸ¬ìš´ ìƒì„±ì— íš¨ê³¼ì )

### ğŸ” ìê¸°íšŒê·€ì  ìƒì„± (Autoregressive Generation)
- **Autoregressive Decoding Algorithm**
  - ì‹œì  *t*ê¹Œì§€ì˜ ì…ë ¥ìœ¼ë¡œë¶€í„° *t+1*ë²ˆì§¸ í† í°ì„ ì˜ˆì¸¡.
  - ì˜ˆì¸¡ëœ í† í°ì„ ì…ë ¥ì— ì¶”ê°€ â†’ ë°˜ë³µ ìˆ˜í–‰.
- í•œ ë‹¨ì–´ì”© ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê¸° ë•Œë¬¸ì— ì‚¬ëŒì´ ê¸€ì„ ì“°ë“¯ ë¬¸ë§¥ì„ ê³ ë ¤í•œ ìì—°ìŠ¤ëŸ¬ìš´ ìƒì„± ê°€ëŠ¥.
- ê·¸ëŸ¬ë‚˜, **ê¸´ í…ìŠ¤íŠ¸ ìƒì„± ì‹œ ì†ë„ ì €í•˜** ë° **ì—°ì‚° ë¹„ìš© ì¦ê°€** ë¬¸ì œê°€ ë°œìƒ.

---

## 2. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± (Simple Text Generation)

### ğŸ§© Hugging Face Pipeline
- Hugging Faceì˜ `pipeline()`ì€ **ëª¨ë¸ ë¡œë“œ â†’ ì…ë ¥ ì²˜ë¦¬ â†’ í›„ì²˜ë¦¬**ë¥¼ ìë™í™”í•œ ê³ ìˆ˜ì¤€ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.
- ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ì§€ì›: Text Generation, Summarization, Translation, Question Answering ë“±.
- ë‚´ë¶€ ë™ì‘:
  1. ì…ë ¥ ì „ì²˜ë¦¬  
  2. ëª¨ë¸ ì¶”ë¡  (inference)  
  3. ê²°ê³¼ í›„ì²˜ë¦¬ ë° ì¶œë ¥  

```python
from transformers import pipeline
generator = pipeline("text-generation", model="gpt2")
print(generator("AI will change the world", max_length=50))
```

### ğŸš§ ê¸°ë³¸ ìƒì„± ë°©ì‹ì˜ í•œê³„ (Naive Decoding Limitation)
- ê¸´ í…ìŠ¤íŠ¸ë‚˜ ëŒ€ëŸ‰ ìš”ì²­ ì‹œ **ë§¤ìš° ëŠë¦¼**.
- ì´ìœ :
  - ìˆœì°¨ì  í† í° ìƒì„±ìœ¼ë¡œ ë§¤ë²ˆ ëª¨ë¸ í˜¸ì¶œ.
  - Attention ì—°ì‚°ì˜ **O(nÂ²)** ë³µì¡ë„.
  - KV Cache ë¹„íš¨ìœ¨ì  ê´€ë¦¬ (CPU ê¸°ë°˜, ë…ë¦½ Kernel ì—°ì‚° ë“±).

---

## 3. ì¶”ë¡  ìµœì í™” (Inference Optimization)

### âš¡ FlashAttention
- **IO ë³‘ëª©**(HBM ë©”ëª¨ë¦¬ ì ‘ê·¼ ë¹„ìš©)ì„ ì¤„ì´ê¸° ìœ„í•´ ê³ ì•ˆëœ íš¨ìœ¨ì  Attention ì—°ì‚°.
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì†ë„ ëª¨ë‘ í¬ê²Œ ê°œì„  (ìµœëŒ€ 2ë°° ì´ìƒ í–¥ìƒ).
- v2 ë²„ì „ì€ ë” ë‚˜ì€ ë³‘ë ¬í™” ë° ì‘ì—… ë¶„í•  êµ¬ì¡°ë¡œ ë°œì „.

ğŸ“„ Reference: *Dao et al., â€œFlashAttentionâ€ (NeurIPS 2022)*

---

### ğŸ§± Paged Attention & KV Caching
- **KV Cache**: ì´ì „ í† í°ì˜ Key/Valueë¥¼ ì €ì¥í•´ ì¤‘ë³µ ì—°ì‚° ë°©ì§€.
- í•˜ì§€ë§Œ Cache í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ë©”ëª¨ë¦¬ ë³‘ëª© ë°œìƒ.
- **Paged Attention**:
  - KV Cacheë¥¼ **Block ë‹¨ìœ„ë¡œ ê´€ë¦¬**í•˜ê³  **Dynamic Memory Allocation** ìˆ˜í–‰.
  - ëŒ€ê·œëª¨ LLM ì„œë¹™ì—ì„œ íš¨ìœ¨ì  ë©”ëª¨ë¦¬ ì‚¬ìš© ê°€ëŠ¥.

ğŸ“„ Reference: *Kwon et al., â€œPagedAttentionâ€ (SOSP 2023)*

---

### ğŸ”® Speculative Decoding
- íš¨ìœ¨ì  ëª¨ë¸(Mq)ì´ ì—¬ëŸ¬ í›„ë³´ í† í°(Î³ê°œ)ì„ ìƒì„±í•˜ê³ ,
  ë” ì •í™•í•œ ëª¨ë¸(Mp)ì´ ì´ë¥¼ í•œ ë²ˆì— í‰ê°€.
- í•œ ë²ˆì˜ Mp í˜¸ì¶œë¡œ **Î³+1ê°œì˜ í† í° ìƒì„±** â†’ ì†ë„ ëŒ€í­ í–¥ìƒ.

ğŸ“„ Reference: *Leviathan et al., â€œFast Inference via Speculative Decodingâ€ (ICML 2023)*

---

### ğŸ” Continuous Batching
- ê¸°ì¡´: ëª¨ë“  ì‹œí€€ìŠ¤ì˜ ì¶”ë¡ ì´ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸° (Batch í¬ê¸° ê³ ì •).
- ê°œì„ : í•œ ì‹œí€€ìŠ¤ê°€ ëë‚˜ë©´ ìƒˆë¡œìš´ ì‹œí€€ìŠ¤ë¥¼ ì¦‰ì‹œ ì¶”ê°€í•˜ì—¬ **Batchë¥¼ ì§€ì†ì ìœ¼ë¡œ ê°±ì‹ **.
- GPU ìì› íš¨ìœ¨ ê·¹ëŒ€í™”, ì²˜ë¦¬ëŸ‰(Throughput) ì¦ê°€.

ğŸ“„ Reference: *Anyscale Blog, 2023*

---

### ğŸ§© Kernel Fusion
- ì—¬ëŸ¬ ì—°ì‚°(MatMul â†’ Add ë“±)ì„ **í•˜ë‚˜ì˜ ì»¤ë„**ë¡œ í•©ì³ ì‹¤í–‰.
- GPU Launch Overheadë¥¼ ì¤„ì´ê³  ì†ë„ í–¥ìƒ.

ğŸ“„ Reference: NVIDIA Developer Blog, *â€œTensorRT Inference Optimizationâ€*

---

## 4. ì¶”ë¡  ì‹œìŠ¤í…œ (Inference Systems)

| ì‹œìŠ¤í…œ | íŠ¹ì§• |
|--------|------|
| **VLLM** | Python ê¸°ë°˜, FlashAttention & PagedAttention í†µí•©, Speculative Decoding, Continuous Batching ì§€ì› |
| **Text-Generation-Inference (TGI)** | Rust ê¸°ë°˜, GPU ë³‘ë ¬ì²˜ë¦¬, ì•ˆì •ì  ì„œë¹™ API ì œê³µ |
| **SGLang** | LLMÂ·VLM ì„œë¹™ ì§€ì›, Structured Output ë° Function Calling ê°€ëŠ¥ |
| **OpenAI Compatible** | OpenAI APIì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ (ChatCompletion í˜•ì‹) |

---

## 5. ì¶”ë¡  ì‹œìŠ¤í…œ ì‚¬ìš© ê°€ì´ë“œ (Practical Guide)

### ğŸ§  VLLM
```bash
pip install vllm
sh vllm.sh  # ëª¨ë¸ ì„œë¹™
python inference.py  # OpenAI-compatible inference
```
- **íŠ¹ì§•**: FlashAttention, PagedAttention, Continuous Batching ëª¨ë‘ ì§€ì›.

### âš™ï¸ TGI (Text-Generation-Inference)
```bash
docker install
sh tgi.sh
python inference.py
```
- **íŠ¹ì§•**: Rust ê¸°ë°˜, Tensor Parallelismìœ¼ë¡œ ë‹¤ì¤‘ GPU íš¨ìœ¨ì  í™œìš©.

### ğŸ§© SGLang
```bash
pip install sglang[all]
pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/
sh sglang.sh
python inference.py
```
- **íŠ¹ì§•**: Multi-modal ëª¨ë¸ ì§€ì›, Function Calling ê°€ëŠ¥.

---

## 6. ë” ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•œ íŒ (Advanced Acceleration)

### ğŸš€ Medusa
- **Multiple Decoding Heads** ë¥¼ í•™ìŠµì‹œì¼œ ì—¬ëŸ¬ ë¯¸ë˜ í† í°ì„ ë™ì‹œì— ìƒì„±.
- Parameter-efficient êµ¬ì¡°ë¡œ ê¸°ì¡´ ëŒ€ë¹„ ë¹ ë¥¸ ì¶”ë¡  ê°€ëŠ¥.

ğŸ“„ Reference: *GitHub - FasterDecoding/Medusa*

---

### ğŸ§® Quantization
- ëª¨ë¸ì˜ **Floating Point**ë¥¼ **int8 / int4** ë“±ìœ¼ë¡œ ë³€í™˜.
- ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ ê°€ëŠ¥ (ì•½ 2~4ë°°).
- ì£¼ë¡œ *LLM.int8()* ë˜ëŠ” *GPTQ* ë°©ì‹ ì‚¬ìš©.

ğŸ“„ Reference: *OpenMMLab (2023), Gaussian37 Blog*

---

## ğŸ“Š ìš”ì•½ (Summary)

| êµ¬ë¶„ | ë‚´ìš© |
|------|------|
| **ê¸°ë³¸ ìƒì„±** | LLMì˜ ìê¸°íšŒê·€ì  ìƒì„± ë°©ì‹, ë””ì½”ë”© ì „ëµ (Greedy, Top-k, Top-p) |
| **í•œê³„** | ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œ ì†ë„ ì €í•˜ ë° ë©”ëª¨ë¦¬ ë³‘ëª© |
| **ìµœì í™” ê¸°ë²•** | FlashAttention, PagedAttention, KV Caching, Speculative Decoding ë“± |
| **ì¶”ë¡  ì‹œìŠ¤í…œ** | VLLM, TGI, SGLang â€” ëª¨ë‘ OpenAI-Compatible ë°©ì‹ ì§€ì› |
| **ê³ ì†í™” íŒ** | Medusa (ë³‘ë ¬ ë””ì½”ë”©), Quantization (ëª¨ë¸ ê²½ëŸ‰í™”) |

> ğŸ’¡ ë‹¤ì–‘í•œ ì‹œìŠ¤í…œì„ OpenAI API í˜•íƒœë¡œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°,  
> Latency, Throughput, ì´ˆë‹¹ í† í° ìƒì„± ìˆ˜ ë“± ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ìœ¼ë¡œ ë¹„êµí•´  
> í”„ë¡œì íŠ¸ í™˜ê²½ì— ë§ëŠ” ìµœì ì˜ ì¶”ë¡  ë°©ì‹ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤.

---

ğŸ“š **References**
- Dao et al., *FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness* (NeurIPS 2022)  
- Kwon et al., *PagedAttention: Efficient Memory Management for LLM Serving* (SOSP 2023)  
- Leviathan et al., *Fast Inference via Speculative Decoding* (ICML 2023)  
- NVIDIA, *TensorRT Inference Optimization*  
- [vLLM Docs](https://docs.vllm.ai/en/latest)  
- [Hugging Face TGI](https://huggingface.co/docs/text-generation-inference/index)  
- [SGLang GitHub](https://github.com/sgl-project/sglang)  
- [Medusa Project](https://github.com/FasterDecoding/Medusa)  
- [Quantization Guide](https://openmmlab.medium.com/faster-and-more-efficient-4-bit-quantized-llm-model-inference-a27d35a66c29)
