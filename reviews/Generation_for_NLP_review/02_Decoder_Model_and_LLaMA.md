# 02_Decoder-Model_and_LLaMA.md

---

## ğŸ§© 1. Decoder-only Model Review and Intro to LLaMA

Decoder-only ëª¨ë¸ì€ **Auto-Regressive êµ¬ì¡°**ë¥¼ í†µí•´ ì´ì „ í† í°ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ  
ë‹¤ìŒ í† í°ì„ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.  
ëŒ€í‘œì ì¸ ì˜ˆì‹œëŠ” **GPT ì‹œë¦¬ì¦ˆ**ì´ë©°, ìƒì„±í˜• ì–¸ì–´ëª¨ë¸ì˜ ê¸°ë°˜ì´ ë©ë‹ˆë‹¤.

### ğŸ”¹ Decoder-only ëª¨ë¸ì˜ 3ëŒ€ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

#### 1ï¸âƒ£ Auto-Regressive (ìê¸°íšŒê·€)
- **ì˜ë¯¸:** ì´ì „ì— ìƒì„±ëœ í† í°ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡  
- **ì¥ì :** ë¬¸ë§¥ ì¼ê´€ì„±ê³¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ìƒì„±ì— ìœ ë¦¬  
- **ì˜ˆì‹œ:** ë¬¸ì¥ì„ ì™„ì„±í•˜ê±°ë‚˜ ìŠ¤í† ë¦¬ë¥¼ ì´ì–´ê°€ëŠ” ìƒì„± ì‘ì—…ì— íš¨ê³¼ì   

#### 2ï¸âƒ£ Self-Attention (ìê¸° ì£¼ì˜)
- **ì˜ë¯¸:** ì…ë ¥ ì‹œí€€ìŠ¤ ë‚´ ëª¨ë“  í† í° ê°„ì˜ ê´€ê³„ë¥¼ ê³ ë ¤  
- **ì‘ë™:** ê° í† í°ì´ ë‹¤ë¥¸ ëª¨ë“  í† í°ê³¼ì˜ ê´€ë ¨ì„±ì„ ê³„ì‚°í•˜ì—¬ ì¤‘ìš”ë„ë¥¼ íŒŒì•…  
- **ì¥ì :** ê¸´ ê±°ë¦¬ì˜ ì˜ì¡´ì„±(Long-range dependency)ì„ í¬ì°© ê°€ëŠ¥ â†’ ë¬¸ë§¥ ì´í•´ë ¥ í–¥ìƒ  

#### 3ï¸âƒ£ Causal Masking (ì¸ê³¼ì  ë§ˆìŠ¤í‚¹)
- **ì˜ë¯¸:** í˜„ì¬ í† í°ì´ ë¯¸ë˜ í† í° ì •ë³´ë¥¼ ì°¸ì¡°í•˜ì§€ ëª»í•˜ë„ë¡ ì œí•œ  
- **ì‘ë™:** ìê¸° ì£¼ì˜ ì—°ì‚° ì‹œ, í˜„ì¬ ì´í›„ í† í°ì˜ attention ì ìˆ˜ë¥¼ ë§ˆìŠ¤í‚¹(0ìœ¼ë¡œ ì„¤ì •)  
- **ëª©ì :** ë¯¸ë˜ ì •ë³´ë¥¼ ë¯¸ë¦¬ ì•Œì§€ ëª»í•˜ê²Œ í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ ìƒì„± ìœ ë„  

---

## ğŸ¦™ 2. LLaMA ëª¨ë¸ ì†Œê°œ

### ğŸ“– LLaMAë€?
- **Meta AI**ì—ì„œ ê³µê°œí•œ **ì˜¤í”ˆì†ŒìŠ¤ LLM ì‹œë¦¬ì¦ˆ**  
- **Auto-Regressive Decoder-only êµ¬ì¡°** ê¸°ë°˜  
- **LLaMA 1 â†’ 2 â†’ 3 â†’ 3.1 â†’ 3.2**ë¡œ ë°œì „  
- ìµœì‹  ë²„ì „(LLaMA 3.2)ì€ **í…ìŠ¤íŠ¸ + ë¹„ì „ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸**

### ğŸ”¹ LLaMA ëª¨ë¸ì˜ ì²˜ë¦¬ ìˆœì„œ

1. **Input Token Embedding**  
   â†’ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ì…ë ¥ ì„ë² ë”©)  
2. **RMS Normalization**  
   â†’ ì…ë ¥ì„ ì •ê·œí™”í•˜ì—¬ ì•ˆì •ì ì¸ í•™ìŠµ ìœ ë„  
3. **Rotary Positional Embedding (RoPE)**  
   â†’ Q, Kì— ìœ„ì¹˜ì •ë³´ë¥¼ íšŒì „(rotation) ë°©ì‹ìœ¼ë¡œ ì£¼ì…  
4. **Multi-Head Attention (FlashAttention2)**  
   â†’ ë³‘ë ¬ self-attentionìœ¼ë¡œ ê´€ê³„ íŒŒì•… ë° íš¨ìœ¨ í–¥ìƒ  
5. **RMS Normalization (2nd)**  
6. **Feed-Forward Network (SwiGLU í™œì„±í™” í•¨ìˆ˜)**  
7. **ì¶œë ¥ì¸µ: Linear + Softmax**  
   â†’ ë‹¤ìŒ í† í° í™•ë¥  ë¶„í¬ ì˜ˆì¸¡

---

## âš™ï¸ 3. LLaMAì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ (Key Components)

| êµ¬ì„± ìš”ì†Œ | ì„¤ëª… | ì£¼ìš” ì—­í•  |
|------------|------|------------|
| **RoPE (Rotary Positional Embedding)** | ê° í† í°ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ íšŒì „ í˜•íƒœë¡œ ì¸ì½”ë”© | ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´ ë³´ì¡´ |
| **Multi-Head Attention** | ì—¬ëŸ¬ ê´€ì (head)ì—ì„œ ê´€ê³„ íŒŒì•… | ë¬¸ë§¥ì  ì—°ê²° ê°•í™” |
| **Feed-Forward Network (FFN)** | ë¹„ì„ í˜• ë³€í™˜ì„ í†µí•´ íŠ¹ì§• í™•ì¥ | í‘œí˜„ë ¥ í–¥ìƒ |
| **RMS Normalization** | í‰ê·  ëŒ€ì‹  ì œê³±í‰ê· ìœ¼ë¡œ ì •ê·œí™” | ê³„ì‚° íš¨ìœ¨ì„± ë° ì•ˆì •ì„± |
| **FlashAttention2** | GPU ìµœì í™”ëœ ì–´í…ì…˜ ê³„ì‚° | ë©”ëª¨ë¦¬ ì ˆê° ë° ì†ë„ í–¥ìƒ |
| **SwiGLU Activation** | ReLU ê°œì„ í˜• í™œì„±í™” í•¨ìˆ˜ | ë¹„ì„ í˜• í•™ìŠµ ì„±ëŠ¥ í–¥ìƒ |

---

## ğŸ§  4. Code-Level Analysis of LLaMA

> HuggingFace Transformers ë‚´ `modeling_llama.py` ê¸°ì¤€ìœ¼ë¡œ êµ¬ì„±ìš”ì†Œë³„ ì‘ë™ ë°©ì‹ ì •ë¦¬

---

### ğŸ”¸ 4.1 Rotary Positional Embedding (RoPE)

**ê°œë…:**  
í† í°ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ â€œíšŒì „(rotation)â€ìœ¼ë¡œ í‘œí˜„í•˜ì—¬ ìƒëŒ€ì  ìœ„ì¹˜ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜.

**ì‘ë™ ì›ë¦¬:**  
1. ê° í† í°ì˜ ìœ„ì¹˜ì— ë”°ë¼ íšŒì „ ê°ë„(Î¸)ë¥¼ ê³„ì‚°  
2. ì„ë² ë”© ë²¡í„°ì˜ ì ˆë°˜ì„ cos(Î¸), ë‚˜ë¨¸ì§€ë¥¼ sin(Î¸)ë¡œ íšŒì „  
3. Query(Key)ì— RoPEë¥¼ ì ìš© â†’ Attention ì‹œ ìœ„ì¹˜ì •ë³´ ë°˜ì˜

**ë¹„ìœ :**  
> ì‹œê³„ ë°”ëŠ˜ì´ ì‹œê°„ì— ë”°ë¼ íšŒì „í•˜ë“¯, í† í°ë„ ë¬¸ì¥ ë‚´ ìœ„ì¹˜ì— ë”°ë¼ íšŒì „ ê°ë„ê°€ ë‹¬ë¼ì§.  
> ë‘ í† í°ì˜ ê°ë„ ì°¨ì´ê°€ ë°”ë¡œ â€œê±°ë¦¬ ì •ë³´â€ë¥¼ ì˜ë¯¸í•¨.

---

### ğŸ”¸ 4.2 Multi-Headed Attention

**ê°œë…:**  
ì…ë ¥ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ â€œë¨¸ë¦¬(head)â€ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ê´€ê³„ë¥¼ í•™ìŠµ.

**ì‘ë™ ìˆœì„œ:**  
1. Query, Key, Value í–‰ë ¬ ìƒì„±  
2. RoPEë¥¼ Queryì™€ Keyì— ì ìš©  
3. Attention Score ê³„ì‚° (Q Ã— K^T / âˆšd_k)  
4. Softmax í›„ Valueì™€ ê³±í•´ ìµœì¢… ì¶œë ¥ ìƒì„±  
5. ì—¬ëŸ¬ Head ê²°ê³¼ë¥¼ ê²°í•©í•´ í’ë¶€í•œ í‘œí˜„ë ¥ í™•ë³´

**ë¹„ìœ :**  
> ì—¬ëŸ¬ ì „ë¬¸ê°€ê°€ ê°ê°ì˜ ì‹œì ì—ì„œ ë¬¸ì¥ì„ ì½ê³ ,  
> ê·¸ë“¤ì˜ ì˜ê²¬ì„ ì¢…í•©í•´ ë” ê¹Šì´ ìˆëŠ” í•´ì„ì„ ë§Œë“œëŠ” ê³¼ì •ê³¼ ìœ ì‚¬.

---

### ğŸ”¸ 4.3 Feed-Forward Network (FFN / MLP)

**êµ¬ì¡°:**  
- ì™„ì „ì—°ê²°ì¸µ 2ê°œ  
- ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ (SwiGLU) ì‚¬ìš©  
- ì…ë ¥ì„ ë” í° ì°¨ì›ìœ¼ë¡œ í™•ì¥ â†’ ë‹¤ì‹œ ì¶•ì†Œ (Gate êµ¬ì¡°)

**ê¸°ëŠ¥:**  
- Attentionì´ í•™ìŠµí•œ ê´€ê³„ë¥¼ í•´ì„í•˜ê³ ,  
- ë” ë³µì¡í•œ ë¹„ì„ í˜• íŒ¨í„´ì„ í•™ìŠµ

---

### ğŸ”¸ 4.4 RMS Normalization

**ì •ì˜:**  
- ì…ë ¥ì„ í‰ê·  ëŒ€ì‹  **Root Mean Square** ê°’ìœ¼ë¡œ ì •ê·œí™”  
- LayerNormì˜ ê°„ì†Œí™” ë²„ì „  
- ê³„ì‚°ëŸ‰ â†“, ì•ˆì •ì„± â†‘

**ìˆ˜ì‹:**  
\[
RMSNorm(x) = \frac{x}{\sqrt{mean(x^2)}}
\]

**íŠ¹ì§•:**
- í‰ê·  ëº„ì…ˆ ê³¼ì • ìƒëµ â†’ ë” ë¹ ë¦„  
- ë°°ì¹˜ í¬ê¸°ì— ëœ ë¯¼ê°  
- T5 ëª¨ë¸ì˜ LayerNormê³¼ ê±°ì˜ ë™ì¼  

---

### ğŸ”¸ 4.5 Flash Attention 2

**ëª©ì :**  
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì ˆê°  
- ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ì†ë„ í–¥ìƒ

**í•µì‹¬ ê¸°ìˆ :**
- **Tiling:** í° í–‰ë ¬ì„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë‚˜ëˆ  GPU ê³ ì† ë©”ëª¨ë¦¬(SRAM)ì—ì„œ ê³„ì‚°  
- **Recomputation:** ì¤‘ê°„ê²°ê³¼ ì €ì¥ ëŒ€ì‹  í•„ìš” ì‹œ ì¬ê³„ì‚°  

**FlashAttention2 ê°œì„ ì :**
- Që¥¼ ë¶„í• í•˜ê³  K, Vë¥¼ ê³µìœ  â†’ ë™ê¸°í™” ê°ì†Œ, ë³‘ë ¬ì„± í–¥ìƒ  
- ê¸°ì¡´ ëŒ€ë¹„ **2~3ë°° ë¹ ë¥¸ ì—°ì‚° ì†ë„**

**LLaMA ë‚´ êµ¬í˜„:**  
- `LlamaAttention` í´ë˜ìŠ¤ ìƒì†  
- FlashAttention APIë¡œ query, key, value ê³„ì‚°  
- RoPE ì ìš© í›„ attention ê²°ê³¼ ë°˜í™˜  

---

## ğŸ§¾ Reference Links

- [LLaMA Architecture Overview](https://devopedia.org/llama-llm)  
- [Rotary Embedding (RoFormer)](https://arxiv.org/pdf/2104.09864)  
- [Attention is All You Need](https://arxiv.org/abs/1706.03762)  
- [RMS Normalization (Zhang, 2019)](https://arxiv.org/abs/1910.07467)  
- [Flash Attention 2 (Dao, 2023)](https://arxiv.org/pdf/2307.08691)  
- [HuggingFace LLaMA Source Code](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)

---

**â“’ NAVER Connect Foundation | Boostcamp AI Tech NLP Recent Trends**
