# ğŸ§© 8ê°•. Embedding

## 1ï¸âƒ£ ì„ë² ë”©ì˜ ê°œë…

### 1.1 ì„ë² ë”©ì´ë€?
ì„ë² ë”©(Embedding)ì€ **ì´ì‚°ì (discrete)** ë°ì´í„°ë¥¼ **ì—°ì†ì (continuous)** ë²¡í„° ê³µê°„ìœ¼ë¡œ ì‚¬ì˜(embedding)í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.  
ì¦‰, í† í°í™”ëœ ë¬¸ì¥(ì •ìˆ˜ IDë¡œ í‘œí˜„ëœ ë²¡í„°)ì„ ì˜ë¯¸ë¥¼ ë³´ì¡´í•œ ì‹¤ìˆ˜ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

- ì´ì‚° ë²¡í„° â†’ ì—°ì† ë²¡í„° ë³€í™˜
- ì˜ë¯¸ë¡ ì  ì •ë³´(Semantic Information)ë¥¼ ì¸ì½”ë”©
- í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ë‹¤ì–‘í•œ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜ ë²¡í„°ë¡œ í‘œí˜„

> ì˜ˆì‹œ:  
> "I like natural language processing" â†’ [0.12, 0.69, 0.32, 0.86, 0.31, â€¦]

---

### 1.2 ì„ë² ë”©ì´ í•„ìš”í•œ ì´ìœ 
- ì´ì‚°ì ì¸ í…ìŠ¤íŠ¸ë¥¼ **ì˜ë¯¸ ìˆëŠ” ë²¡í„° ê³µê°„**ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´
- **ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë¬¸ì¥ë¼ë¦¬ ê°€ê¹ê²Œ ìœ„ì¹˜**í•˜ë„ë¡ í•˜ê¸° ìœ„í•´  
- ì˜ˆ: â€œê³ ì–‘ì´â€ì™€ â€œê°•ì•„ì§€â€ëŠ” ë²¡í„° ê³µê°„ìƒ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ë§¤í•‘

> ğŸ”¸ ì´ˆê¸°ì—ëŠ” ë‹¨ì–´ ìˆ˜ì¤€(Word-level) ìœ ì‚¬ë„ë¥¼ í•™ìŠµ (Word2Vec)  
> ğŸ”¸ í˜„ì¬ëŠ” ë¬¸ì¥(Sentence-level)Â·ë¬¸ì„œ(Document-level)ë¡œ í™•ì¥

---

### 1.3 ì„ë² ë”©ì˜ í˜•íƒœ
| í˜•íƒœ | ì„¤ëª… |
|------|------|
| **Dense Representation** | ëª¨ë“  ì°¨ì›ì— ì‹¤ìˆ˜ê°’ì„ ê°–ëŠ” ë°€ì§‘ ë²¡í„° |
| **Low-Dimensional** | ê³„ì‚° íš¨ìœ¨ì„ ìœ„í•´ ì°¨ì›ì„ ì¶•ì†Œ |
| **Semantic Preservation** | ì˜ë¯¸ê°€ ë¹„ìŠ·í•œ ë°ì´í„°ëŠ” ê°€ê¹Œìš´ ë²¡í„° ìœ„ì¹˜ |

---

### 1.4 ì„ë² ë”© ì ìš© ì˜ˆì‹œ
í•™ìŠµëœ ì„ë² ë”© ëª¨ë¸ë¡œ ë¬¸ì¥ì„ ë²¡í„°í™”í•˜ë©´,  
ìœ ì‚¬í•œ ë¬¸ì¥ë“¤ì€ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ì¡´ì¬í•©ë‹ˆë‹¤.

> ì˜ˆì‹œ ì‹œê°í™”: [Semantle-Ko](https://semantle-ko.newsjel.ly/)  
> ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ë¬¸ì¥ì´ í´ëŸ¬ìŠ¤í„°ë§ë˜ì–´ ë‚˜íƒ€ë‚¨.

---

## 2ï¸âƒ£ ì„ë² ë”©ì˜ ì¤‘ìš”ì„±

### 2.1 ì£¼ìš” í™œìš© ë¶„ì•¼

| ë¶„ì•¼ | ì„¤ëª… |
|------|------|
| **IR (Information Retrieval)** | ëŒ€ê·œëª¨ ë¬¸ì„œì—ì„œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰ |
| **QA (Question Answering)** | ì§ˆë¬¸ê³¼ ë‹µë³€ ê°„ ì˜ë¯¸ì  ì—°ê´€ì„± ê³„ì‚° |
| **STS (Semantic Textual Similarity)** | ë‘ ë¬¸ì¥ì˜ ì˜ë¯¸ ìœ ì‚¬ë„ í‰ê°€ |
| **RAG (Retrieval-Augmented Generation)** | ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•´ LLMì˜ ìƒì„± í’ˆì§ˆ í–¥ìƒ |

---

### 2.2 ì„ë² ë”© ì„±ëŠ¥ì˜ ì¤‘ìš”ì„±

#### âœ… ì„±ëŠ¥ì´ ì¢‹ì„ ë•Œ
```
User Question: AI ê¸°ìˆ ì„ í™œìš©í•œ ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ ë°©ë²•ì€?
Similarity Search: AI ê¸°ìˆ ì€ AlphaFold ëª¨ë¸ì„ í†µí•´ ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ì— í˜ì‹ ì„ ë³´ì„.
LLM Response: AI ê¸°ë°˜ AlphaFold ëª¨ë¸ì€ ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ì—ì„œ í° ì§„ì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤.
```

#### âŒ ì„±ëŠ¥ì´ ë‚˜ì  ë•Œ
```
User Question: AI ê¸°ìˆ ì„ í™œìš©í•œ ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ ë°©ë²•ì€?
Similarity Search: AIëŠ” ììœ¨ì£¼í–‰ì°¨ì™€ ê°™ì€ ê¸°ìˆ ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.
LLM Response: AIëŠ” ììœ¨ì£¼í–‰ì°¨, ì´ë¯¸ì§€ ì¸ì‹ ë“±ì—ì„œ ì¤‘ìš”í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.
```

> â†’ ì„ë² ë”© í’ˆì§ˆì€ **ê²€ìƒ‰ ì •í™•ë„, ì‘ë‹µì˜ ì‹ ë¢°ë„(Hallucination ê°ì†Œ)** ì— ì§ê²°ë©ë‹ˆë‹¤.

---

## 3ï¸âƒ£ ì„ë² ë”© ëª¨ë¸ í•™ìŠµ ë°©ë²•

### 3.1 Word2Vec â€” ì´ˆê¸° ì„ë² ë”© ëª¨ë¸
ë‹¨ì–´ ìˆ˜ì¤€ ì„ë² ë”© í•™ìŠµ ëª¨ë¸ë¡œ CBOWì™€ Skip-gram êµ¬ì¡° ì‚¬ìš©.

| ëª¨ë¸ | ë°©ì‹ | íŠ¹ì§• |
|------|------|------|
| **CBOW** | ì£¼ë³€ ë‹¨ì–´ â†’ ì¤‘ì‹¬ ë‹¨ì–´ ì˜ˆì¸¡ | ë¬¸ë§¥ í¬ì°©ì— ê°•í•¨ |
| **Skip-gram** | ì¤‘ì‹¬ ë‹¨ì–´ â†’ ì£¼ë³€ ë‹¨ì–´ ì˜ˆì¸¡ | í¬ê·€ ë‹¨ì–´ í•™ìŠµì— ê°•í•¨ |

> ë‹¨ì : ë¬¸ë§¥ ì •ë³´ê°€ ë¶€ì¡±í•˜ê³ , ë‹¤ì˜ì–´ í‘œí˜„ì´ ì–´ë µë‹¤.

---

### 3.2 Sentence-BERT (Bi-Encoder)
- BERTë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì¥ ìˆ˜ì¤€ ì„ë² ë”©ì„ ìƒì„±
- **Siamese Network êµ¬ì¡°**ë¡œ ë‘ ë¬¸ì¥ì˜ ìœ ì‚¬ë„ë¥¼ ì½”ì‚¬ì¸ ê±°ë¦¬ë¡œ ê³„ì‚°
- **NLI ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµ**, **STS ë°ì´í„°ë¡œ ë¯¸ì„¸ì¡°ì •**

> ì¥ì : ë¹ ë¥¸ ê²€ìƒ‰ ë° í´ëŸ¬ìŠ¤í„°ë§  
> ë‹¨ì : Cross-Encoderì— ë¹„í•´ ì •í™•ë„ê°€ ë‚®ìŒ

---

### 3.3 Contrastive Learning (+ Self-Supervised Learning)
- **ë ˆì´ë¸” ì—†ì´** ìœ ì‚¬ë„ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•
- **ìœ ì‚¬í•œ ë°ì´í„° â†’ ê°€ê¹Œì´**, **ë‹¤ë¥¸ ë°ì´í„° â†’ ë©€ë¦¬**

**í•™ìŠµ ì ˆì°¨**
1. ë°ì´í„° ì¦ê°•ìœ¼ë¡œ Positive Pair ìƒì„±  
2. ë‹¤ë¥¸ ìƒ˜í”Œë¡œ Negative Pair ìƒì„±  
3. ëŒ€ì¡° ì†ì‹¤ í•¨ìˆ˜ (InfoNCE, NT-Xent ë“±) ì‚¬ìš©  

> ëŒ€í‘œ ëª¨ë¸: SimCLR, MoCo, CLIP  
> â†’ ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œë„ ê°•ê±´í•œ í‘œí˜„ í•™ìŠµ ê°€ëŠ¥

---

## 4ï¸âƒ£ ìµœì‹  ì„ë² ë”©

### 4.1 ë§ì´ í™œìš©ë˜ëŠ” ëª¨ë¸
ì£¼ìš” ëª¨ë¸: **E5, BGE, JinaEmbedding**

| íŠ¹ì§• | ì„¤ëª… |
|------|------|
| ì…ë ¥ ê¸¸ì´ ì¦ê°€ | ê¸´ ë¬¸ì¥Â·ë¬¸ì„œ ë‹¨ìœ„ ì„ë² ë”© ê°€ëŠ¥ |
| ëŒ€ê·œëª¨ ë°ì´í„° ê¸°ë°˜ Contrastive Learning | í’ë¶€í•œ ì˜ë¯¸ ì •ë³´ í•™ìŠµ |
| ê³ í’ˆì§ˆ Negative Sampling Fine-tuning | ì„¸ë°€í•œ ì˜ë¯¸ êµ¬ë¶„ ê°€ëŠ¥ |

**SentenceTransformer ì‚¬ìš© ì˜ˆì‹œ**
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
sentences = ['This is an example sentence', 'Each sentence is converted']
embeddings = model.encode(sentences)
print(embeddings)
```

---

### 4.2 ì„ë² ë”© ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
- ëŒ€í‘œ ë°ì´í„°ì…‹: **MTEB**, **BEIR**
- í‰ê°€ ë°©ì‹: ìœ ì‚¬ë„, ê²€ìƒ‰, ë¶„ë¥˜, í´ëŸ¬ìŠ¤í„°ë§ ë“± ë‹¤ì–‘í•œ Task ìˆ˜í–‰

**MTEB ì‚¬ìš© ì˜ˆì‹œ**
```python
from mteb import MTEB
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")
evaluation = MTEB(tasks=["Classification"])
results = evaluation.run(model, output_folder="results")
```

> ğŸ“Š [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

---

### 4.3 LLM ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸

#### âš™ï¸ Encoder ê¸°ë°˜ì˜ í•œê³„
- ë°ì´í„°ì…‹ ìˆ˜ì§‘ì´ ë³µì¡ (Manual Curation)
- ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ex. BERT: 512 tokens)

#### ğŸš€ Decoder ê¸°ë°˜ ëª¨ë¸ì˜ ê°•ì 
- ì˜¤í”ˆì†ŒìŠ¤ LLM í™œìš© â†’ ì¼ë°˜ ì •ë³´ ì²˜ë¦¬ì— ê°•í•¨
- LLMì„ ì„ë² ë”© ëª¨ë¸ë¡œ ì§ì ‘ íŠœë‹ ê°€ëŠ¥
- ìë™ ë°ì´í„° ìˆ˜ì§‘ (OpenAI API ë“±ìœ¼ë¡œ 180M+ í† í° ìƒì„±)

#### í•™ìŠµ ë°©ì‹
1. ë¬¸ì¥ ëì— **EOS í† í° ì‚½ì…**
2. **EOS Hidden States**ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©
3. Contrastive Learningìœ¼ë¡œ ìµœì í™”

> ëŒ€ë¶€ë¶„ì˜ ìƒìœ„ê¶Œ ì„ë² ë”© ëª¨ë¸ì€ **Decoder-Only LLM ê¸°ë°˜**

---

### 4.4 í•œêµ­ì–´ ì„ë² ë”© ì„±ëŠ¥
- **allganize/RAG-Evaluation-Dataset-KO**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ  
  Markerì—ì„œ **í•œêµ­ì–´ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹** ì œì‘  
  ğŸ”— [AutoRAG Example Korean Benchmark](https://github.com/Marker-Inc-Korea/AutoRAG-example-korean-embedding-benchmark)

---

## 5ï¸âƒ£ ì„ë² ë”© ì „í›„ ë¹„êµ

### 5.1 RAG ì„±ëŠ¥ ë¹„êµ
- ì„ë² ë”© ì„±ëŠ¥ì— ë”°ë¼ **RAGì˜ Hallucination, ì‹ ë¢°ë„(Trustworthiness)** ê°€ í¬ê²Œ ë‹¬ë¼ì§.
- ê³ í’ˆì§ˆ ì„ë² ë”©ì€ **ì •í™•í•œ ë¬¸ë§¥ ê²€ìƒ‰ê³¼ ì•ˆì •ì ì¸ ì‘ë‹µ ìƒì„±**ì„ ê°€ëŠ¥í•˜ê²Œ í•¨.

---

## ğŸ“š Reference
- Efficient Estimation of Word Representations in Vector Space (Word2Vec)  
- Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks  
- SimCSE: Simple Contrastive Learning of Sentence Embeddings  
- MTEB: Massive Text Embedding Benchmark  
- BEIR: Zero-shot Evaluation of IR Models  
- Improving Text Embeddings with LLMs  
- [Jina Embeddings v2](https://huggingface.co/jinaai/jina-embeddings-v2-base-code)  
- [Intfloat Multilingual E5](https://huggingface.co/intfloat/multilingual-e5-large)
