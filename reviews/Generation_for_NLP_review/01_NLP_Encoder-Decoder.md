# 01_Encoder-Decoder.md

---

## 1ï¸âƒ£ Encoder-Decoder / Encoder-only ëª¨ë¸

### 1.1 ëª¨ë¸ì˜ ë°œì „ íë¦„

| ì—°ë„ | ì£¼ìš” ëª¨ë¸ | íŠ¹ì§• |
|------|------------|------|
| 2017 | Transformer | Attention ê¸°ë°˜ êµ¬ì¡°ì˜ ë“±ì¥ |
| 2018 | BERT | Encoder ì¤‘ì‹¬ì˜ ì´í•´ ëª¨ë¸ |
| 2019 | GPT-2 | ìƒì„±í˜• Decoder ëª¨ë¸ì˜ ì‹œì‘ |
| 2020 | GPT-3 | ëŒ€ê·œëª¨ Pretrained LM ì‹œëŒ€ |
| 2022~ | ChatGPT, GPT-4/4o | LLM ê¸°ë°˜ ìƒì„±í˜• AI ì‹œëŒ€ ê°œë§‰ |

- Encoder ëª¨ë¸ì€ **NLU(Natural Language Understanding)** ë°œì „ì„ ì£¼ë„  
- Decoder ëª¨ë¸ì€ **Generation ì¤‘ì‹¬ì˜ AI ëª¨ë¸ ì‹œëŒ€**ë¥¼ ì—´ì—ˆìŒ  

---

## 1.2 Encoder-Decoder ëª¨ë¸

### ëŒ€í‘œ ëª¨ë¸
- **Transformer, BART, T5 ë“±**

### êµ¬ì¡° ë° ì‘ë™ ë°©ì‹
- Encoderì—ì„œ ì…ë ¥ ì •ë³´ë¥¼ **í•´ì„**
- Decoderì—ì„œ Cross-Attentionì„ í†µí•´ Encoder ì •ë³´ ê¸°ë°˜ìœ¼ë¡œ **ì¶œë ¥ ìƒì„±**
- ì…ë ¥ì˜ ì˜ë¯¸ë¥¼ ì••ì¶•í•˜ê³ , ì´ë¥¼ ì´ìš©í•´ ìì—°ìŠ¤ëŸ¬ìš´ ìƒì„± ìˆ˜í–‰

### í™œìš© ë¶„ì•¼
- **ê¸°ê³„ ë²ˆì—­ / ìš”ì•½ / Generative QA**
- ìƒì„± Task ìˆ˜í–‰ ì‹œ Decoder-only ëª¨ë¸ì˜ ë¶€ì¡±í•œ ë¶€ë¶„ ë³´ì™„

### í•œê³„
- **íš¨ìœ¨ì„± ë¬¸ì œ**: Self-Attention + Cross-Attentionìœ¼ë¡œ ì¸í•œ ê³„ì‚° ë³‘ëª©  
- **í™•ì¥ì„± í•œê³„**: In-Context Learning ìµœì í™”ì— ì¶”ê°€ ë¹„ìš© ë°œìƒ

---

## 1.3 Encoder-only ëª¨ë¸

### ëŒ€í‘œ ëª¨ë¸
- **BERT (Masked Language Model)**  
  â†’ ë¬¸ì¥ ë‚´ [MASK] í† í°ì„ ì˜ˆì¸¡í•˜ì—¬ ë¬¸ë§¥ ì´í•´ ëŠ¥ë ¥ ê°•í™”

### íŠ¹ì§•
- ë¬¸ë§¥ ì´í•´ ë° ë¶„ë¥˜(Classification) Taskì— ê°•ì   
- ë¬¸ì¥ ë‚´ ê´€ê³„ ì´í•´ì— íƒì›”

### í™œìš© ë¶„ì•¼
- ë¬¸ì„œ ë¶„ë¥˜(ê°ì • ë¶„ì„, ìŠ¤íŒ¸ í•„í„°ë§)
- ì •ë³´ ê²€ìƒ‰(IR, Embedding)
- Extractive QA (ì§€ë¬¸ ë‚´ì—ì„œ ì •ë‹µ ì¶”ì¶œ)

### í•œê³„
- Sequential Task (ìƒì„±í˜•)ì— ì•½í•¨  
  â†’ Auto-Regressive êµ¬ì¡°ê°€ ì•„ë‹˜  
  â†’ **ìƒì„± Task ìˆ˜í–‰ ë¶ˆê°€**

---

## 2ï¸âƒ£ Decoder-only ëª¨ë¸

### 2.1 GPT (Causal Language Model)
- ì´ì „ í† í°ê³¼ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ **ë‹¤ìŒ í† í°(Next Token)** ì˜ˆì¸¡  
- ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ìƒì„±ì— íƒì›”  
- **ìƒì„± Task** ì¤‘ì‹¬ì˜ ëª¨ë¸ êµ¬ì¡°

### 2.2 ë°œì „
- **Emergent Abilities**:  
  ëª¨ë¸ í¬ê¸°ì™€ í•™ìŠµëŸ‰(Training FLOPs)ì´ ì¼ì • ìˆ˜ì¤€ì„ ë„˜ìœ¼ë©´  
  **Few-Shot Learning, Reasoning** ë“± ìƒˆë¡œìš´ ëŠ¥ë ¥ì´ ìë°œì ìœ¼ë¡œ ë“±ì¥

### 2.3 ì„±ëŠ¥
- Generation Taskì—ì„œ ë§¤ìš° ë›°ì–´ë‚¨  
- NLU Taskì—ì„œë„ ì¶©ë¶„íˆ ë†’ì€ ì„±ëŠ¥  
- **MMLU Benchmark**: ìƒìœ„ ëª¨ë¸ ëŒ€ë¶€ë¶„ì´ Decoder-only êµ¬ì¡° ì‚¬ìš©

---

## 3ï¸âƒ£ LLM ì‹œëŒ€

### 3.1 LLM(Decoder-only)ì˜ ì¥ì  ë° ì„±ëŠ¥

**ì¥ì **
- Encoder-Decoderë³´ë‹¤ íš¨ìœ¨ì  (í•™ìŠµ/ì¶”ë¡  ì†ë„ ë¹ ë¦„)
- Scaling Upì— ìœ ë¦¬
- In-Context Learningì„ í†µí•´ ì…ë ¥ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„± ëŠ¥ë ¥ ê°•í™”

**ì„±ëŠ¥**
- ëŒ€ë¶€ë¶„ì˜ ìƒì„± ë° ì´í•´ Taskë¥¼ ìˆ˜í–‰ ê°€ëŠ¥  
- ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œë„ ë‹¤ì–‘í•œ ë³´ì¡° ì—­í•  ìˆ˜í–‰ (ì˜ˆ: ChatGPT)

---

### 3.2 In-Context Learning (ICL)

- **Few-shot Learning**: í”„ë¡¬í”„íŠ¸ì— ì†Œìˆ˜ ì˜ˆì‹œë§Œìœ¼ë¡œë„ ì‘ì—… ìˆ˜í–‰
- **Chain-of-Thought (CoT)**: ë³µì¡í•œ ì¶”ë¡  ê³¼ì •ì„ ë‹¨ê³„ì ìœ¼ë¡œ í‘œí˜„
- Standard Promptingë³´ë‹¤ ë†’ì€ ì„±ëŠ¥
- ëª¨ë¸ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ í–¥ìƒ í­ ì¦ê°€

**ì˜ˆì‹œ (ê°ì • ë¶„ì„ Prompting)**  



---

### 3.3 Human Alignment

**Instruction Tuning**
- ë‹¤ì–‘í•œ ì§€ì‹œì‚¬í•­(instruction)ì„ í¬í•¨í•œ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ì¡°ì •  
- ëª¨ë¸ì´ ì¼ë°˜ì ì¸ ëª…ë ¹ì„ ì´í•´í•˜ê³  ë”°ë¥´ë„ë¡ í•™ìŠµ  
- ì˜ˆì‹œ: FLAN, T5

**RLHF (Reinforcement Learning with Human Feedback)**
- ì¸ê°„ í”¼ë“œë°±ì„ í†µí•´ ëª¨ë¸ ì‘ë‹µì˜ í’ˆì§ˆì„ ê°œì„   
- ê³¼ì •:
  1. ì´ˆê¸° ì‘ë‹µ ìƒì„±  
  2. ì¸ê°„ì˜ ì„ í˜¸ë„ í‰ê°€  
  3. ë³´ìƒ ëª¨ë¸ í•™ìŠµ  
  4. ì •ì±… ìµœì í™”
- ChatGPTê°€ ì´ ë°©ì‹ìœ¼ë¡œ ë°œì „

---

### 3.4 LLMì˜ ì„±ëŠ¥
- Pretraining + Instruction Tuning + RLHFë¥¼ í†µí•´  
  ê³ í’ˆì§ˆì˜ ìì—°ì–´ ì´í•´ ë° ìƒì„± ëŠ¥ë ¥ì„ ë‹¬ì„±  
- Chatbot Arena ë“±ì—ì„œ ëª¨ë¸ ê°„ ì„±ëŠ¥ ë¹„êµ ê°€ëŠ¥  
  ğŸ‘‰ [https://chat.lmsys.org/](https://chat.lmsys.org)

---


## ğŸ“š ì°¸ê³  ë…¼ë¬¸ ë° ë§í¬

- [BART](https://arxiv.org/pdf/1910.13461)  
- [Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682)  
- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)  
- [Multi-task Language Understanding on MMLU](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)  
- [Chain-of-Thought Prompting](https://arxiv.org/pdf/2201.11903)  
- [Instruction Tuning](https://arxiv.org/pdf/2109.01652)  
- [RLHF (Direct Preference Optimization)](https://arxiv.org/pdf/2305.18290)

---

**â“’ NAVER Connect Foundation | Boostcamp AI Tech NLP Recent Trends**
